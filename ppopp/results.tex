\section{Results} \label{sresults}

\subsection{Parallelism in Cilk programs}

\begin{table}
\small
\begin{tabular}{ | l | l | l | }
\hline
Program & Description & Parameters \\
\hline
cholesky & Matrix decomposition & \texttt{size=256}, \texttt{nonzeros=1000} \\
cilksort & Merge sort & \texttt{size=100000} \\
fft & Fourier transform & \texttt{size=512*512} \\
fib & Na\"ive Fibonacci calculation & \texttt{n=30} \\
heat & Differential equation solver & \texttt{nx=ny=512}, \texttt{nt=1} \\ 
lu & Matrix decomposition & \texttt{n=256} \\
magic & Magic squares search & \texttt{n=4} \\
matmul & Matrix multiplication & \texttt{n=128} \\
plu & Matrix decomposition & \texttt{n=128} \\
strassen & Matrix multiplication & \texttt{n=512} \\
\hline
\end{tabular}
\caption{Description and parameters for Cilk 5.4.6 examples used}
\label{cilk-ex}
\end{table}

We begin by presenting the results of analysing example Cilk programs packaged with the 5.4.6 release of Cilk, as described in Table~\ref{cilk-ex}\footnote{We have omitted programs that use the \texttt{inlet}, \texttt{abort} and \texttt{SYNCHED} keywords, as their translation into ordinary C is not straightforward.}.
As examples packaged with a parallel programming environment, these are programs known to have lots of task-level parallelism.
We ran the original programs in Cilk a number of times to obtain a figure for average parallelism as calculated by the Cilk infrastructure.
We then translated these programs into semantically equivalent programs in ordinary C simply by stripping the Cilk keywords\footnote{Namely, \texttt{cilk}, \texttt{spawn} and \texttt{sync}.}.
These translated programs are then analysed with our extension of Embla under a number of different models.

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-run}
 \caption{Parallelism of Cilk programs as calculated by Cilk and Embla}
 \label{cilk-run}
\end{figure}

Figure~\ref{cilk-run} compares the parallelism found by Cilk (averaged over 60 runs) to that found by our extension of Embla on their `decilkified' versions.
Our baseline model for this comparison uses agreegated data dependences and control dependences, and considers only function-call-level parallelism without loop parallelization or spawn hoisting---the closest model to the one used in Cilk.
We can observe that our tool is able to find all of the task-level parallelism in most of the original Cilk programs.
In fact, using dependences discovered by our tool it is easy to re-insert Cilk keywords into the program to materialise the parallelism discovered.
The only exceptions are fib and magic, the Cilk parallelism for which appears to vary greatly.
The reason our tool does not find much parallelism in magic compared to the Cilk run is because the Cilk program uses an associative reduction variable in a loop which sums over the results of each iteration, in the form \texttt{for (...) count += spawn(...);}.
Our tool currently cannot recognise associative reduction variables, and therefore cannot discover such parallelism.

\begin{figure}
  \begin{center}
  \small
  \begin{subfloat}
    \begin{minipage}{1.4in}
      \begin{verbatim}
spawn f1();
spawn f2();
spawn f3();
spawn f4();
sync;
spawn g1();
spawn g2();
spawn g3();
spawn g4();



      \end{verbatim}
    \end{minipage}%
    \label{without}
    \caption{Best possible parallelization with universal \texttt{sync}s}
  \end{subfloat}%
  \qquad
  \begin{subfloat}
    \begin{minipage}{1.4in}
      \begin{verbatim}
fut1 = spawn f1();
fut2 = spawn f2();
fut3 = spawn f3();
fut4 = spawn f4();
sync fut1;
spawn g1();
sync fut2;
spawn g2();
sync fut3;
spawn g3();
sync fut4;
spawn g4();
      \end{verbatim}
    \end{minipage}%
    \label{with}
    \caption{Best possible parallelization with individual \texttt{sync}s}
  \end{subfloat}%
  \end{center}
  \caption{An example of the greater parallelism caused by individual \texttt{sync}s.
      \texttt{g1} depends only on \texttt{f1}, \texttt{g2} on \texttt{f2} etc.. }
  \label{cilk-sync}
\end{figure}


In fact, for a few examples our tool can discover more parallelism than explicitly specified in the original Cilk program.
We found functions called sequentially in cholesky, heat and strassen that could have been spawned.
In addition, we also found C library function calls in heat, lu and plu that cannot be spawned directly in Cilk (as library functions are not defined as spawnable) but can be spawned with the addition of simple wrappers.
The greater parallelism found in cholesky, lu and plu can also be partly attributed to a restriction in Cilk that \texttt{sync}s must join on all tasks spawned rather than any individual task.
If tasks could be synchronised on individually, as illustrated in Figure~\ref{cilk-sync} then greater parallelism may be found.

We now look more closely at the various models used to examine whether they affect potential parallelism in these programs.

\subsubsection{Data dependences}

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-data}
 \caption{Parallelism of Cilk programs with aggregated and exact data dependences}
 \label{cilk-data}
\end{figure}

Figure~\ref{cilk-data} shows the potential parallelism of our `decilkified' programs with aggregated and exact data dependences.
In all programs but lu we see that the difference between the two models is negligible,
This suggests that for most programs most of the potential parallelism is achievable by adding parallel constructs at the source level---runtime techniques such as thread-level speculation appear to give few performance benefits.

\subsubsection{Control dependences}

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-ctl}
 \caption{Parallelism of Cilk programs with and without control dependences}
 \label{cilk-ctl}
\end{figure}

The effects of control speculation are shown in Figure~\ref{cilk-ctl}, which compares the parallelism when control dependences are considered to that when they are ignored.
For these programs we can see that control speculation does not have any effect on available parallelism.
One possible reason for this is that most of these programs are stream-processing-like, with few branches that depend on the data input.
More irregular programs may see greater improvements with control speculation.

\subsubsection{Granularity}

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-gran}
 \caption{Parallelism of Cilk programs with different levels of granularity}
 \label{cilk-gran}
\end{figure}

The amount of line-level parallelism in these programs is compared to the amount of task-level parallelism in Figure~\ref{cilk-gran}.
We can see that for most of these programs the amount of line-level parallelism is around twice or more the amount of function-call-level parallelism.
Most of this difference is due to simple statements that perform arithmetic operations inside a function call or loop that can run in parallel.
Each of these operations takes a small number of cycles, which means that it is not viable for each of these to be spawned.
Nevertheless, operations may be grouped and extracted into tasks that are sufficiently large to see performance gains.
We also note that some of this parallelism may well be realised already in existing superscalar or VLIW processors.

\subsubsection{Loops}

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-loop}
 \caption{Parallelism of Cilk programs with and without spawning loop iterations}
 \label{cilk-loop}
\end{figure}

Looking at Figure~\ref{cilk-loop}, which compares the potential parallelism of Cilk programs with and without spawning loop iterations, we can see that the use of parallel for-loops benefit most of the programs considered here.
This is most remarkable in matmul, where there is a 64-fold gain in parallelism when for loops are parallelised.
We deduce from this that the use of parallel for-loops is an excellent tool for expressing task-level parallelism, in addition to function calls.
Indeed, this validates the inclusion of parallel for-loops in Cilk++, the commercialised version of Cilk.
Our extension of Embla not only finds the amount of loop-level parallelism in a program, but can be used to easily identify candidate loops for parallelisation.
It can simply be done by searching through the dependences output by our tool for dependences between iterations of the loop concerned.
If there are no such dependences, then the loop can be parallelised.

\subsubsection{Spawn hoisting}

\begin{figure}
 \centering
 \includegraphics[width=3in]{cilk-hoist}
 \caption{Parallelism of Cilk programs with and without spawn hoisting}
 \label{cilk-hoist}
\end{figure}

Figure~\ref{cilk-hoist} shows little effect of spawn hoisting on the amount of potential parallelism in the Cilk examples.
This suggests that most function calls are already spawned at the earliest possible point in the program, and there is little further hoisting possible.

\subsection{Parallelism in various benchmarks}

\begin{figure*}
 \centering
 \begin{subfloat}
 \includegraphics[width=6.5in]{spec}
 \caption{SPEC CPU 2000 integer benchmarks}
 \label{spec}
 \end{subfloat}

 \begin{subfloat}
 \centering
 \includegraphics[width=6.5in]{specfp}
 \caption{SPEC CPU 2000 floating point benchmarks}
 \label{specfp}
 \end{subfloat}

 \begin{subfloat}
 \centering
 \includegraphics[width=6.5in]{mb}
 \caption{miBench}
 \label{mb}
 \end{subfloat}
\caption{Parallelism of three benchmark suites under various models}
\label{benchmarks}
\end{figure*}

We also ran the same analysis using our tool on some of benchmark programs in the SPEC CPU 2000 (with the MinneSPEC reduced data input set \cite{KleinOsowski02minnespec}) and miBench suites, the results of which are displayed in Figure~\ref{benchmarks}.
As before, the baseline model uses agreegated data dependences and control dependences, and considers only function-call-level parallelism without loop parallelization or spawn hoisting.
Each of the other models differs from the baseline by one parameter described in Section~\ref{smethod}, except for Ultimate, which considers line-level parallelism by using exact data dependences, ignoring control dependences and hoisting spawns.
In general, we see that few benchmarks exhibit the level of parallelism seen in the Cilk examples.
In fact, none of these benchmarks exhibit parallelism of over 3 under the baseline model.

There are, however, some benchmarks with a significant amount of loop-level parallelism.
One of them is the miBench benchmark SUSAN, a program that performs image smoothing, corner detection and edge detection on an image, and is data-parallel---the same computation is performed on each pixel in the image and the results for each pixel are independent of each other.
This is reflected in our results, which show that both susan.corners and susan.edges have potential parallelism of over 12 when loop iterations are spawned.
The same would have been true for susan.smoothing had it not had an associative reduction variable in the loop, which as mentioned our tool currently cannot recognise.

We note also that for some programs, e.g.\ eon, spawning loop iterations actually results in a \emph{lower} level of parallelism.
This is because that while spawning loop iterations allows us to exploit DOALL parallelism, where loop iterations are completely independent from each other and can be executed in parallel, it precludes DOACROSS parallelism or software pipelining, where there are cross-iteration dependences but loop iterations can still partially overlap.
The balance between DOALL and DOACROSS loops would therefore determine whether parallelism rises or falls compared to the baseline.

\subsection{Discussion}

Our results suggest that while the example programs from Cilk have lots of inherent task-level parallelism, most general-purpose programs tend to have little and cannot be transformed into highly concurrent programs simply by spawning existing function calls and loops.
Nevertheless, our extension of Embla can output the critical path of each function call, allowing us to examine the bottlenecks that prevent greater parallelism from being realised.
Having examined several programs in more detail, we now present some of the reasons why they exhibit such low levels of parallelism, and suggest what can be done to increase their potential.

\subsubsection{186.crafty}

Crafty is a chess-playing program that uses a Minimax-like algorithm to find the best move through searching a game tree.
In theory game tree searching is an easily parallelisable activity, as demonstrated by the success of multi-threaded chess-playing programs \cite{Dailey01usingcilk}.
However, our tool does not find plenty of parallelism in Crafty for a number of reasons.
Firstly, Crafty uses one global chessboard, which is updated when a move is considered and then reverted afterwards, creating a chain of data dependences that linearise the search.
Furthermore, various pruning techniques used mean that later searches are influenced by the results of earlier ones.
This shows that algorithmic changes are required before such a program can be parallelized.

\subsubsection{164.gzip}

Gzip is a file compression program.
It works by traversing the input file sequentially, looking for recurrences of substrings that have been seen before.
As such it is easy to see that the program cannot process the later parts of the file before the earlier parts have been processed---this results in little task-level parallelism.
One could imagine changing the algorithm such that a file is divided into blocks that are compressed independently, resulting in greater parallelism at the cost of a larger output file.

\subsubsection{Input/output}

We find that input/output forms a large part of several benchmarks.
In dijkstra, for example, a tenth of the program's sequential execution time is taken up by calls to \texttt{scanf}.
In FFT (miBench), printing the results takes up around 80\% of processing time.
If we assume that input/output is unparallelisable, Amdahl's law would mean that the maximum speed-up, even if we were able to parallelize the rest of the program perfectly, would still be low.
This suggests that for some of the benchmarks examined, a parallel implementation of input/output would be very useful.

\subsubsection{Getting more parallelism from loops}

\begin{figure}
  \centering
  \begin{subfloat}
    \begin{minipage}{3in}
      \begin{verbatim}
for (j = n = 0, seed = rand();
     j < iterations;
     j++, seed += 13) {
  int r = pBitCntFunc[i](seed);
  n += r;
}
      \end{verbatim}
    \end{minipage}%
    \label{orig}
    \caption{Original program}
  \end{subfloat}%
\\
  \begin{subfloat}
    \label{dnc-trans}
    \begin{minipage}{3in}
      \begin{verbatim}
int dc_bitcnts(int start, int n_itrs, int i) {
  if (n_itrs <= 0) return 0;
  if (n_itrs == 1) return pBitCntFunc[i](start);
  else {
    int x,y;
    int half_itrs = n_itrs / 2;
    x = dc_bitcnts(start, half_itrs, i);
    y = dc_bitcnts(start+(half_itrs*13),
          n_itrs - half_itrs, i);
    return x+y;
  }
}

n = dc_bitcnts(seed, iterations, i);
      \end{verbatim}
    \end{minipage}%
    \caption{Transformed program}
  \end{subfloat}%
  \caption{Tranformation of a loop in the bitcount program using divide and conquer.}
  \label{dnc}
\end{figure}

Despite the relatively low figures of parallelism, there are some simple ways of modifying a program to unlock greater parallelism, one of which we present here.
Figure~\ref{dnc} shows a transformation applied to a (slightly adapted) loop in the bitcount benchmark.
In the original program, calls to \texttt{pBitCntFunc[i]} are pure and can be run in parallel with each other.
However, there is a dependence between increments of the induction variables, \texttt{j} and \texttt{seed}, and between additions to the accumulator \texttt{n}, producing two long chains of data dependences.
Nevertheless, by recognising that \texttt{n} is a reduction variable we can transform the loop into recursive calls using a divde-and-conquer strategy.
The lengths of the dependence chains on the induction and reduction variables become logarithmic on the number of iterations instead of linear, and consequently the critical path found by our tool is a tenth of that in the original program, despite the overheads of extra function calls.
This shows that simple transformations can sometimes be sufficient to increase the amount of potential parallelism in certain programs.
In fact, we note that Cilk++, the commercialised version of Cilk, indeed uses a divide-and-conquer strategy for its parallel for-loops, a decision which is justified by our example.

