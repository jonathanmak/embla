% Stuff from the IEEE LaTeX template

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
% \pagestyle{empty}

\usepackage{epic}
\usepackage{color}
\usepackage{verbdef}
\usepackage{alltt}
\usepackage{latexsym}

\newcommand{\comment}[1]{\textit{[ #1 ]}}
\newenvironment{comment_env}
  {\begin{itshape}}
  {\end{itshape}}

\begin{document}

\title{Embla -- Data Dependence Profiling for Parallel Programming }
%\subtitle{Extended Abstract}
\author{Karl-Filip Fax\'en, Konstantin Popov, Sverker Janson, 
       Lars Albertsson\thanks{The author is now employed by Google.}\\
       Swedish Institute of Computer Science\\
       Box 1263, SE-164 29 Kista, Sweden\\
       \{kff,kost,sverker,lalle\}@sics.se}
\date{}
\maketitle

\begin{abstract}

With the proliferation of multicore processors, there is an urgent need for
tools and methodologies supporting parallelization of existing
applications.  In this paper, we present a novel tool for aiding
programmers in parallelizing programs. The tool, Embla, is based on the
Valgrind framework, and allows the user to
discover the data dependences in a sequential program, thereby exposing
opportunities for parallelization.  Embla performs an off-line dynamic 
analysis, and records dependences as they
arise during program execution.  It reports an optimistic view of
parallelizable sequences, and ignores dependences that do not arise during
execution.  
Moreover, since the tool instruments the machine code of the program,
it is largely language independent. 

Since Embla finds the dependencies that occur for particular
executions, the confidence one would assign to its results depend on
whether different executions yield different (bad) or largely the same
(good) dependencies. We present a preliminary investigation into this
issue using 84 different inputs to the SPEC CPU2006 benchmark 403.gcc.
The results indicate that there is a strong correlation between coverage
and finding dependencies; executing the entire program is likely to 
reveal all dependencies.
\end{abstract}

% Intro start ---------------------

\section{Introduction}

Parallel programming is no longer optional.  In order to enjoy continued
performance gains with future generation multicore processors,
application developers must parallelize all software, old and
new~\cite{TEL95,ONHWC96,KAB03,VIAVAC05}.  For scalable parallel
performance, program execution must be divided into large numbers of
independent tasks that can be scheduled on available cores and hardware
threads by runtime systems.  For some classes of programs, static
analysis and automatic parallelization is feasible~\cite{KA02}, but with
the current state-of-the-art, most software requires manual
parallelization.  Our work aims to help developers find the potential
for parallelism in programs by providing efficient tool
support.  In this paper, we present a data dependence profiling approach
to the parallelization problem, an efficient algorithm to project data
dependences onto relevant parts of the program code, and its
implementation, the tool Embla, as well as an analysis of the dependencies 
observed when running the GCC compiler on different inputs.

\begin{figure}
\small
\hrulefill
\[
\begin{minipage}[t]{3cm}
\begin{alltt}
   p();
   q();
   r();
\end{alltt}
\end{minipage}
\begin{minipage}[t]{3cm}
\begin{alltt}
   spawn p();
   q();
   sync;
   r();
\end{alltt}
\end{minipage} 
\]
\hrulefill
\caption{Example of fork-join parallelism.}
\label{fforkjoin}
\end{figure}

We are interested in the following methodology for constructing
parallel programs: Start from a sequential program, identify
independent parts of that program (here Embla can be used) and rewrite
the program to obtain parallel execution of the independent parts.
We focus on introducing
fork-join parallelism~\cite{Conway63}, a framework used
in many parallel programming environments~\cite{BJKLR96, Lea00, DM98, LF00}.

Consider the program fragment in
Figure~\ref{fforkjoin} (left):
Suppose that the calls to {\tt p()} and {\tt q()} are independent,
but that the call to {\tt r()} depends on the earlier calls. Then
the call to {\tt p()} can be
executed in parallel with
the call to {\tt q()}, as shown to the right.
Here we assume the availability of a construct {\tt spawn} to start
the call in parallel and {\tt sync} to wait for all {\tt spawn}'d
activities to terminate (cf. Cilk~\cite{BJKLR96}).
% As long as {\tt p()} and {\tt q()} are
% independent, the parallel program will produce identical results to
% the sequential version.  Therefore it is sufficient to understand
% (debug, verify, \ldots) the sequential program; everything except
% performance carries over to the parallel version.

Embla aims at helping programmers find independent parts of the 
program code.  The availability of independent program parts depends on
the algorithms used and can be further
limited by sequential programming artifacts, such as re-use of
variables and sequential book-keeping in an otherwise parallelizable
algorithm.  Data dependence information can help
identify and remove such obstacles to parallel execution, but this
will not be further discussed here.

Parallelizing compilers mostly target loop parallelization based on
static data dependence analysis methods~\cite{KA02}.  Such analyzers
are by necessity conservative, and use approximations that are always
safe.  Analyzing more general code, e.g., with pointers, remains a
major challenge. For unsafe languages like C, correctness of the
analysis results is only guaranteed for well behaved programs; an
out-of-bounds array index can yield program behaviour not predicted by
the analysis. Consequently, it has proved difficult to parallelize
programs automatically, and most production codes are either written
in an explicitly parallel way or rely on speculative, run-time
parallelization techniques~\cite{PO03,CL03}.

In contrast, Embla
observes the actual data dependences that occur during program
execution, projects them onto relevant program parts, and interprets the
lack of a runtime data dependence as an indication that the program
parts involved are likely to be independent.
Developers will be responsible for selecting
program inputs that generate representative program executions with
good coverage. Section~\ref{sgcc} presents an analysis of how the
dependencies found varies with different inputs.

The methodology mentioned above preserves the
semantics and determinacy of the sequential program under the
assumption that all dependencies are found.  Should a dependence
remain undetected, it might manifest itself as a difference in
behavior between the parallel and sequential versions of the program
for some input. Given that the sequential program is deterministic,
rerunning it under Embla with the offending input will yield the
missing dependence.  Thus, the programming methodology supported by
Embla replaces the problem of finding synchronization errors in the
parallelized, nondeterministic, program with the problem of finding
all of the relevant dependencies in the sequential program.
This is in contrast to tools such as the Intel Thread Checker that
observes fundamentally nondeterministic executions of already 
parallelized programs.

In addition to data dependences, Embla can be extended to
detect I/O and control dependences that limit parallelism.  Another potential
extension is to measure the possible speed
improvement for parallelizing independent program parts and produce a
report with suggested program transforms that will yield the maximum
benefit.  Moreover, Embla presents rich and detailed information; in order
for the parallelizable code sections to be clearly visible, it must be
processed and presented concisely.  These extensions are future work.
This paper
presents the mechanism for collecting runtime data dependence information.

% End intro --------------------

% Begin using -----------------

\section{Using Embla}

\begin{figure} 
\small
\input{ex7.depgraph.ed}
\caption{Code with dependence graph} \label{ffirstex}
\end{figure}

To get a feeling for what dependence profiling is and what Embla can do, 
let us turn to the (contrived) example program in Figure~\ref{ffirstex},
where we see, from left to right, line numbers, data dependence 
arrows and source lines. 

A {\em data dependence} is a pair
of references, not both reads, to overlapping memory
locations with no intervening write. We will refer to these
references as the {\em endpoints} of the dependence.
For instance, in the figure, 
there is an arrow from line 13 to line 14 corresponding to
the assignment to {\tt q} (the {\em early} endpoint) followed by its use 
as an argument in {\tt inc(q)} (the {\em late} endpoint). Embla
distinguishes between flow (RAW), anti (WAR) and output (WAW) 
dependences and is able to present them to the user, but we do not make
that distinction in this paper. 
% Embla can be instructed to show dependence types, which can be
% useful for figuring out the reasons for individual dependences.

The endpoints of the dependence arrow discussed above are parts of
the code for {\tt main} itself, but Embla also tracks references made 
in function calls. For
instance, there is a flow dependence from line 14 to line 16
representing the write in the first invocation of {\tt inc} to the 
{\tt malloc}'d area pointed to by {\tt q} and the subsequent read 
of the same location by a later invocation of {\tt inc}. 
These dependences 
are reported as pertaining to {\tt main} rather than {\tt inc},
although the endpoints are part of the latter function. 
But the importance of the dependence is that, in {\tt main}, the calls
on line 14 and 16 can not be made in parallel.

The dependence given with a dotted arrow 
(from line 13 to line 18) is due to manipulation of administrative 
data structures by {\tt malloc}. If taken at face value such dependences will
serialize all calls to {\tt malloc}, but fortunately, the exact order
of memory allocations is irrelevant. Embla maintains a suppression list,
with functions that behave 
in this way.  Similarly, dependences arise due to the
manipulation of the call stack, which are also irrelevant for parallelization.

% End using ------------------

% Begin algorithm -------------------

\section{Computing Dependencies}   \label{snca}

\begin{figure} \small
\hrulefill
\[
\begin{picture}(160,60)(70,15)
\put(120,65){\makebox(60,10)[c]{\it A:\ \tt main}}
\put(150,65){\line(-2,-1){50}}
\put(150,65){\line( 0,-1){25}}
\put(150,65){\line( 2,-1){50}}
\put(95,45){\makebox(20,10)[r]{\it 14}}
\put(150,45){\makebox(20,10)[l]{\it 15}}
\put(185,45){\makebox(20,10)[l]{\it 16}}
\put(70,50){\makebox(20,10)[r]{\ldots}}
\put(210,50){\makebox(20,10)[l]{\ldots}}
\put(170,30){\makebox(60,10)[c]{\it D:\ \tt inc}}
\put(120,30){\makebox(60,10)[c]{\it C:\ \tt inc}}
\put(70,30){\makebox(60,10)[c]{\it B:\ \tt inc}}
\put(70,15){\makebox(60,10)[cb]{{\tt *q=}\ \ldots}}
\put(170,15){\makebox(60,10)[cb]{\ldots\ {\tt *q}\ \ldots}}
\end{picture}
\]
\hrulefill
\caption{Part of the execution tree of Example 1, edges are annotated 
with the line number of the corresponding call.} 
\label{ffextree}
\end{figure}

Embla maintains an {\em execution tree} that captures full context for
every function call. Every execution tree node corresponds to an
individual function call, and the path from the node to the root of the
tree corresponds to the call stack at the moment of the call. For
example, Figure~\ref{ffextree} depicts a fragment of the execution tree
for our example, capturing the calls of {\tt inc} and lines 14, 15 and
16 from {\tt main}.

Embla traces dependences between individual instructions in the binary
code during program execution. The endpoints of these dependences belong
in general to different function calls. For instance, in
Figure~\ref{ffextree} the read {\tt *q} in the call of {\tt inc} at line 16
of {\tt main} (node {\it D}) depends on the write {\tt *q=...} in the call
of {\tt inc} at line 14 (node {\it B}).

Every dependence between two source lines in a single function call,
like the one between lines 14 and 16 in our example, is caused by an
instruction-level dependence. Embla computes the source-level dependences
from the instruction-level ones using the execution tree as follows. For
every instruction-level dependence, Embla identifies the function calls
where the endpoints occurred (nodes {\it B} and {\it D} for the example
above), and computes the {\em nearest common ancestor} node (NCA) of
those nodes in the execution tree. The NCA corresponds to a function
call with two instructions that are dependent because of
the instruction-level dependence (lines 14 and 16 in {\tt main} in the
example).

\newcommand{\tracepile}{trace pile}

%\subsection{Computing dependences}

Embla uses two main data structures: The {\em \tracepile}, which implements 
the execution tree, and the {\em memory table} which maps addresses to tree
nodes corresponding to the last write and subsequent reads of that
location. The \tracepile\ contains the part of the execution tree
corresponding to the part of the instruction trace that has been
seen so far.
For each reference, we look up the data address in the memory 
table. If the reference is a read, we use the previous write to generate
a flow dependence (RAW). If it is a write, we use the previous write to 
construct an output dependence (WAW) as well as all reads since that 
write to construct anti dependences (WAR).

If there are several reads with no intervening write, a subsequent write
(anti) depends on all of them. Since the reads do not depend on each other,
we need to keep track of all of them in the memory table to generate the
anti dependence edges explicitly. When that write has been processed 
the read list can be deallocated since
the write depends on all of the reads and
all subsequent references depend on the write.

The \tracepile\ contains the nodes of the execution tree in the same order
as in the instruction trace. For each node {\tt n}, 
% {\tt n.line} is the source line associated with the instruction 
% (leaf) or procedure call (internal) corresponding to {\tt n}, 
{\tt n.parent} is the parent node in the execution tree (there is also 
other information associated with a node, such as what source line 
corresponds to the node).
% and, if {\tt n} is 
% on the path between the root node of the tree and the most recent 
% event (leaf node), {\tt n.next} is the last child of {\tt n} (the node 
% one step closer to the most recent one
% along that path), so {\tt n.next.parent} = {\tt n}. This
% path corresponds to the call stack; {\tt n.next} is the stack frame on
% top of {\tt n}.
The NCA is computed by following the {\tt parent} links, starting at the
early endpoint, until a node corresponding to an activation record currently 
on the call stack is found. This is the NCA since the late endpoint, and hence
all of its ancestors, are on the stack.
Path compression can be used to speed up this computation.
Every node {\tt n} that is visited but is not on the stack can have its 
{\tt parent} field set
to its closest ancestor on the stack. We conjecture that this reduces the 
complexity of the algorithm to essentially constant time. 

We can do better than path compression by {\em compacting} the \tracepile. 
Once a procedure call has returned, we will not distinguish between 
different events in the subtree corresponding to its (completed) 
execution. They will all be represented by the root node of the subtree
(the call instruction).
We periodically compact the \tracepile, replacing subtrees
corresponding to completed calls by their root nodes. Since the 
memory table contains pointers into the \tracepile, compaction
entails forwarding these pointers to the root nodes of the compacted 
subtrees.
After compaction, the \tracepile\ contains the
tree nodes corresponding to the stack and their immediate children,
with subtrees abridged to just the call and return events.

After forwarding, pointers to previously distinct events in the same read list 
now may point at the same 
tree nodes. In this case it is unnecessary to represent more than one 
copy of each pointer, thus compacting the read lists. This optimization is 
crucial in practice.

\subsection{Prototype implementation}

Embla uses the Valgrind instrumentation infrastructure which emulates
the user mode instruction set architecture. An interseting alternative
would be to sample dependencies using hardware data breakpoints
(Accumem's Virtual Performance Expert uses this approach for cache
profiling, but since single dependencies are much more important than
single cache misses, it is not clear if sampling is suitable for
dependence analysis).

Our examples of profiling
output use C, but
the profiling is done at instruction level, and the result is
mapped to source level using debugging information.




% End algorithm -----------------------

\begin{table} \small
\begin{center} \newcommand{\sms}{\hskip0.4em}
\begin{tabular}{|@{\sms}l@{\sms}|r@{\sms}r|r@{\sms}r@{\sms}r|r@{\sms}r@{\sms}r|} \hline
 & & & \multicolumn{3}{|c|}{No RLC} & 
\multicolumn{3}{c|}{RLC} \\
Prog & \#L & \#sD & \#iD & RSz & T 
                              & \#iD & RSz & T \\ \hline
\tt ex & 17 & 15 & 3.7K & 12K & 0.8 & 3.7K & 12K & 0.8 \\
\tt fib & 22 & 7 & 32M & 14K & 6.7 & 32M & 14K & 6.9 \\
\tt qs & 79 & 83 & 82M & 39M & 24.7 & 82M & 35M & 22.7 \\
\tt mpeg & 6053 & 3330 & 3.3G & 2.0G & 2355 & 3.2G & 109M & 847 \\ \hline
\end{tabular}
\end{center}
RLC: Read List Compaction, 
\#L: \#non blank source lines, \#sD: \#source dependences, \#iD: 
\#instruction
level dependences, RSz: max bytes for read lists, T: run time (s)
\caption{Some experimental results} \label{trex}
\end{table}

\section{Preliminary Experiments}

We have run some preliminary experiments to verify that the tool performs as
expected. Some results are reported in Table~\ref{trex}. The programs are 
{\tt ex}, our example from Figure~\ref{ffirstex}, {\tt fib}, a recursive
Fibonacci implementation, {\tt qs}, a recursive quicksort implementation 
and {\tt mpeg}, an MPEG encoder~\cite{MPEG} encoding 10 frames.

It is interesting
to see that read list compaction has a very different effect on different 
programs depending on the frequency of long sequences of reads from the same 
location (the RSz columns), ranging from no difference for {\tt ex} and 
{\tt fib} to a 20x difference for {\tt mpeg}. We also see that the number of 
instruction 
level dependences (the \#iD columns) are affected since the read lists 
are shorter when a write occurs following compaction (however, the 
eliminated read list 
items would have yielded no new source level dependences).

We note that Embla finds the expected independence of the recursive calls 
in the recursive divide-and-conquer programs {\tt qs} and
{\tt fib}, even though {\tt qs} is an in-place version coded 
with pointers and pointer arithmetic, something that is well known to be 
difficult to deal with for static analyzers. 
% We have also done naive
% hand parallelization of these programs according to the information 
% yielded by Embla and obtained the expected good speedup.
% Need backup for this statement.  What was the speedup?

\begin{table*} \small
\[
\begin{tabular}{|l|rrrr@{\ }r|rrr@{\ }rr|rrrrr|} \hline
Filter & \multicolumn{5}{c|}{No. of Dependencies ($\#D_i$)} 
     & \multicolumn{5}{c|}{Deltas relative to $\cal A$ ($\#D_i^{\ast}$)}
     & \multicolumn{5}{c|}{Deltas relative to random covering $R$} \\
     & max & min & avg & total & \% of N
     & max & avg & total & \% of N & \%$\emptyset$ 
     & max & avg & total & \% of N & \%$\emptyset$ 
     \\ \hline \hline
\multicolumn{16}{|c|}{ All 403.gcc source files } \\ \hline
N & 91374 & 11653 & 57828  & 125041 &  & 774 & 111  & 9321 &  & 11 & 171 & 28.5 & 1536 & & 4 \\
T & 55676 & 7509 & 36751  & 82750 & 66.2  & 601 & 58  & 4857 & 52.1  & 15 & 30 & 6.2 & 372 & 24.2 & 6 \\
\hline \hline
\multicolumn{16}{|c|}{ {\tt c-parse.c}, {\tt combine.c}, 
  {\tt insn-recog.c}, and {\tt toplev.c} } \\
\hline
N & 23676 & 1298 & 12679 & 38807 &      & 479 & 55 & 4597 &      & 14 & 143 & 22.8 & 1185 &      &  7 \\
T &  9948 & 1060 &  5562 & 19141 & 49.3 & 249 & 21 & 1785 & 38.8 & 24 &  21 &  4.1 &  240 & 20.3 & 13 \\
C & 11612 &  514 &  6048 &  8472 & 21.8 &  94 &  8 &  654 & 14.2 & 55 &   8 &  0.6 &   30 &  2.5 & 73 \\
CT & 3823 &  417 &  2272 &  5865 & 15.1 &  90 &  6 &  465 & 10.1 & 64 &   2 &  0.1 &    7 &  0.6 & 92 \\
\hline
\end{tabular}
\]
Filters are {\bf N}one, {\bf T}ransitive or {\bf C}ontrol flow, \# Dependencies and Size of deltas
give maximum, minimum and average sizes of $D_i$ over $i\in \cal A$
as well as sizes of $\cup_{i \in \cal A} D_i$. \% of N gives 
the total for the row as a percentage of the total of the N row. 
For dependencies relative to $\cal A$ and to random covering $R$ we give 
maximum (the minimums are 0 in all cases) and average sizes of $D_i^{\ast}$ and $D_i^R$, respectively,
 over $i\in \cal A$
as well as sizes of $\cup_{i \in \cal A} D_i$.\% of N gives 
the total for the row as a percentage of the total of the N row. $\%\emptyset$ gives the percentage
of cases where $D_i^{\ast}$ and $D_i^R$, respectively, were empty.
\caption{Dependence statistics} \label{tdepstat}
\end{table*}

\subsection{Analysis of dependencies in GCC} \label{sgcc}

We are ultimately interested in knowing how well the dependencies
collected during testing predict the dependencies encountered in
production runs.  Since the sequential program we test is
deterministic, the issue becomes how well the behavior under the
testing inputs predict the behavior under other inputs.

We have studied the SPEC CPU 2006
benchmark 403.gcc, a variant of the Gnu C
Compiler. This is a very challenging program since, being an
optimizing compiler, it essentially looks for a large set of patterns
in its input an applies different code to different patterns.

We have collected data dependencies for 84 different files
(6--6328 lines of pre-processed C code) drawn from the code base of
403.gcc itself. These dependencies do not include dependencies due to
{\tt malloc} and friends.

Each dependence is a triple $(f, l_1 \leftarrow l_2)$
meaning that $l_1$ in source file $f$ depends on line
$l_2$ in the same file (recall that a dependence is always a
pair of lines in the same function). 
% We only look at forward dependencies where 
% $l_1>l_2$. Backward dependencies occur only in loops and the current
% Embla prototype does not handle loop-carried dependencies adequately.
% In particular, Embla does not recognize induction variables, hence
% reporting all loops as having loop-carried dependencies. Loop bodies 
% may still be parallelized using only forward dependencies.

The 84 inputs yield a total of 125041 dependencies, which
can be compared to the 484930 C source lines (excluding headers) of
this build of GCC. Evidently, the dependencies are rather sparse due
in part to the large number of lines that do not contain memory references at all.
The number of unique source lines that occur in some dependence is
51323 (hence lines that occur in dependencies do so in on the average
about 2.4 lines). % , another measure of their sparseness).

To quantify the effect of differences between inputs on the set of 
dependencies found, we compute {\em dependency deltas}; sets of
dependencies that are generated by some input but not with some
reference set of inputs. Thus, if the dependencies found with input data set $i$ is
$D_i$, we study \[D_j^I = D_j \setminus \bigcup_{i \in I} D_i\] for
some set of inputs $I$ not containing $j$. In particular, we have
$D_j^\ast = D_j^{{\cal A} \setminus j}$ where $\cal A$ is the set of
all the 84 inputs.

Table~\ref{tdepstat} shows some statistics on the dependencies found
in 403.gcc. From the max column of the first row in the relative to $\cal A$ group of columns
we see that at most 774
dependencies in any $D_i$ are preserved in $D_i^{\ast}$; these are the dependencies 
that are only provoked by input $i$. The $\emptyset$ column shows that 11\%, or 9 out of the
84 $D_i^{\ast}$ were empty, meaning that these inputs generated no dependencies
that were not generated by other inputs.

Looking at the dependencies in the $D_i^{\ast}$ sets, we noticed that many of them were 
transitively implied by others. That is, for many $(f, l_1 \leftarrow l_2) 
\in D_i^{\ast}$ there were $(f, l_1 \leftarrow l), (f, l \leftarrow l_2) \in D_i$ for some $l$.
These dependencies give the same constraints on parallelization and make $(f, l_1 \leftarrow l_2)$
redundant. The T filter removes these redundant dependencies, yielding a 34\% reduction in 
overall dependence numbers and a 48\% reduction in the $D_i^{\ast}$ sets (second row).

\begin{figure} \small
\hrulefill
\begin{alltt}
{\it 1:}   if( cond1 )
{\it 2:}     a();
{\it 3:}   else
{\it 4:}     b();
{\it 5:}   if( cond2 )
{\it 6:}     c();
{\it 7:}   else \{
{\it 8:}     d();
{\it 9:}     e();
{\it 10:}  \}
\end{alltt}
\hrulefill
\caption{Control flow analysis example} \label{fcfa}
\end{figure}

We next noticed many data dependencies that were subsumed by control
dependencies (which Embla currently does not capture). In the model we
aim at, only code with the same control dependence can be executed in 
parallel (statements belonging to the same block).
To get an idea
of the magnitude of this contribution, we implemented an approximate
control flow analysis by hand for four modules of 403.gcc that were
top contributors to the $D_i^{\ast}$ sets and appeared to have
nontrivial control flow. 
Based on this information we moved dependence endpoints up in the
syntax tree in a fashion analogous to the NCA computation discussed in
section~\ref{snca}. 

Consider the example in figure~\ref{fcfa} where we have two 
sequential if-statements: 
% \begin{itemize}
% \item
% $4 \leftarrow 2$, a dependence between line 2 and line 4 (the then 
% and else branch of the same if-statement) would be removed by control flow 
% filtering since it has no effect on possible parallelization.
% \item
$6 \leftarrow 2$, (a 
dependence between the then branches)
is remapped to $5 \leftarrow 1$ (between the if-statements
themselves) since line 2 and line 6 do not have the same control dependence.
% \item
Similarly, $5 \leftarrow 2$ becomes $5 \leftarrow 1$.
% \item
$9 \leftarrow 8$ is however not remapped since they have the same control
dependence and could be executed in parallel. 
% in the absence of a $9 \leftarrow 8$ data dependence.
% \end{itemize}

The results are given in the four bottom rows of Table~\ref{tdepstat}
where we focus exclusively on dependencies $(f,l_1 \leftarrow l_2)$ where f is
one of the four control flow analysed 
modules. We see from the first row that these modules
account for almost half of the size of the $D_i^{\ast}$ sets. Looking
at the rightmost column, we see that control flow filtering alone makes
about 55\% of the $D_i^{\ast}$ sets empty, and that together with
transitive filtering almost two thirds are eliminated.


\begin{figure*}
\input{plot.nocf} \input{plot.cf.4}
\caption{Coverage deltas (x-axis) versus dependence deltas (y-axis)
for no (left) or CT filtering} \label{fplot}
\end{figure*}

To account for the remaining $D_i^{\ast}$ sets we turned to coverage
analysis using the {\tt gcov} tool. We form $L_i$ and $L_i^{\ast}$
sets containing lines executed in the same way as for dependencies
above. Figure~\ref{fplot} plots the size of $L_i^{\ast}$ along the
x-axis and $D_i^{\ast}$ along the y-axis. The left hand plot shows the
unfiltered data for all modules whereas the right hand one shows the
data for the CT-filtered four modules. The left hand plot shows a very
clear correlation between execution coverage and dependence
coverage whereas the right hand plot is less conclusive; it will be
interesting to see the effect of control flow filtering of all
modules once we automate it. 
However, all of the 30 inputs that yield empty $L_i^{\ast}$ sets have 
empty $D_i^{\ast}$ sets under CT filtering!

To further investigate the relation between execution coverage and 
dependence coverage we randomly selected 300 pairs $(i,R)$  such that 
\begin{itemize}
\item % where 
$i\in \cal A$ is an input and $R \subseteq {\cal A} \setminus \{i\}$ is a 
subset of the inputs and
\item % such that 
$L_i^R = \emptyset$ (the inputs in $R$ 
make 403.gcc execute all code that $i$ makes it execute). 
\end{itemize}
The average size of the $R$ sets was 28. For each pair 
$(i,R)$ we then computed $D_i^R$, the dependencies provoked by $i$ but 
not by any $j\in R$. The smaller these sets are, the better. The results are in the 
last group of columns in the table. % ; the average size of the $R$ sets was 28.

The combination of dependence remapping based on control flow analysis
and coverage is very effective; table~\ref{tdepstat} shows that in 92\% of
the cases, execution coverage implied dependence coverage. To get an 
intuition about why, consider again the example in figure~\ref{fcfa}.
Suppose that input $i$ yields the path 1,2,5,6 (both conditions true)
and the dependence $6 \leftarrow 2$
(between the then-branches). Similarly, suppose that $R=\{j\}$ where $j$ 
yields paths 1,2,5,8,9 (true, then false) and 1,4,5,6 (false, then true)
with dependence $8 \leftarrow 2$ from the first of these paths. We now have
$L_i = \{1,2,5,6\} \subseteq \{1,2,4,5,6,8,9\} = L_j$ so $L_i^R$ is empty, 
as required, whereas $D_i^R$ contains $6 \leftarrow 2$. Control flow analysis
remaps both dependencies to $5 \leftarrow 1$, making $D_i^R$ empty. In a sense,
this filter bridges part of the gap between line coverage and path coverage.

\section{Conclusion}

We have presented Embla, a data dependence profiling tool, discussed
the main issues and the algorithms used to resolve them. We argue that
data dependence profiling is a useful and practical complement to
static analysis for parallelization of new and legacy code. Finally,
we have reported on a few initial experiments with Embla showing a
strong correlation between executing the same part of a program and
generating the same dependencies.

In the future we plan extensions and refinements of Embla as well as
the application to real legacy code and 
more parallelization experiments using parallel environments such as
OpenMP.


\bibliographystyle{plain}
\bibliography{embib}

\end{document}
