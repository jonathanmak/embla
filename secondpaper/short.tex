% Stuff from the IEEE LaTeX template

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
% \pagestyle{empty}

\usepackage{epic}
\usepackage{color}
\usepackage{verbdef}
\usepackage{alltt}
\usepackage{latexsym}

\newcommand{\comment}[1]{\textit{[ #1 ]}}
\newenvironment{comment_env}
  {\begin{itshape}}
  {\end{itshape}}

\begin{document}

\title{Embla -- Data Dependence Profiling for Parallel Programming }
%\subtitle{Extended Abstract}
\author{Karl-Filip Fax\'en, Konstantin Popov, Sverker Janson\\
       Swedish Institute of Computer Science\\
       Box 1263, SE-164 29 Kista, Sweden\\
       \{kff,kost,sverker\}@sics.se}
\date{}
\maketitle

\begin{abstract}

With the proliferation of multicore processors, there is an urgent need for
tools and methodologies supporting parallelization of existing
applications.  In this paper we present a novel tool for aiding
programmers in parallelizing programs. The tool, Embla, is based on the
Valgrind framework, and allows the user to
discover the data dependences in a sequential program, thereby exposing
opportunities for parallelization.  Embla performs a dynamic analysis,
and records dependences as they
arise during program execution.  It reports an optimistic view of
parallelizable sequences, and ignores dependences that do not arise during
execution.  
Moreover, since the tool instruments the machine code of the program,
it is largely language independent. 

Since Embla finds the dependencies that occur for particular
executions, the confidence one would assign to its results depend on
whether different executions yield different (bad) or largely the same
(good) dependencies. We present a preliminary investigation into this
issue using 88 different inputs to the SPEC CPU2006 benchmark 403.gcc.
The results indicate that there is a strong correlation between coverage
and finding dependencies; executing the entire program is likely to 
reveal all dependencies.
\end{abstract}

% Intro start ---------------------

\section{Introduction}

Parallel programming is no longer optional.  To enjoy continued
performance gains with future generation multicore processors,
application developers must parallelize all software, old and
new~\cite{TEL95,ONHWC96,KAB03,VIAVAC05}.  For scalable parallel
performance, program execution must be divided into large numbers of
independent tasks that can be scheduled on available cores and hardware
threads by runtime systems.  For some classes of programs, static
analysis and automatic parallelization is feasible~\cite{KA02}, but with
the current state-of-the-art, most software requires manual
parallelization.  Our work aims to help developers find the potential
for parallelism in programs, in particular by providing efficient tool
support.  In this paper, we present a data dependence profiling approach
to the parallelization problem, an efficient algorithm to project data
dependences onto relevant parts of the program code, and its
implementation, the tool Embla, as well as an analysis of the dependencies 
observed when running the GCC compiler on different inputs.

\begin{figure}
\small
\hrulefill
\[
\begin{minipage}[t]{3cm}
\begin{alltt}
   p();
   q();
   r();
\end{alltt}
\end{minipage}
\begin{minipage}[t]{3cm}
\begin{alltt}
   spawn p();
   q();
   sync;
   r();
\end{alltt}
\end{minipage} 
\]
\hrulefill
\caption{Example of Fork/join parallelism.}
\label{fforkjoin}
\end{figure}

We are interested in the following methodology for constructing
parallel programs: Start from a sequential program, identify
independent parts of that program (here Embla can be used) and rewrite
the program to obtain parallel executioon of the independent parts.

We will focus on introducing
fork-join parallelism. The fork-join framework was first introduced
by Conway~\cite{Conway63} and is used in many parallel
programming environments,
including Cilk~\cite{BJKLR96}, the Java fork/join
framework~\cite{Lea00}, OpenMP~\cite{DM98}, and Filaments~\cite{LF00}.

Consider the program fragment in
Figure~\ref{fforkjoin} (left):
Suppose that the calls to {\tt p()} and {\tt q()} are independent,
but that the call to {\tt r()} depends on the earlier calls. Then
the call to {\tt p()} can be
executed in parallel with
the call to {\tt q()}, as shown to the right.
Here we assume the availability of a construct {\tt spawn} to start
the call in parallel and {\tt sync} to wait for all {\tt spawn}'d
activities to terminate (cf.~\cite{BJKLR96}).
% As long as {\tt p()} and {\tt q()} are
% independent, the parallel program will produce identical results to
% the sequential version.  Therefore it is sufficient to understand
% (debug, verify, \ldots) the sequential program; everything except
% performance carries over to the parallel version.

Embla can help programmers find independent parts of the 
program code. 
The availability of such independent program parts depends on
the algorithms used and can be further
limited by sequential programming artifacts, such as re-use of
variables and sequential book-keeping in an otherwise parallelizable
algorithm.  Data dependence information can help
identify and remove such obstacles to parallel execution, but this
will not be further discussed here.

Parallelizing compilers mostly target loop parallelization based on
static data dependence analysis methods~\cite{KA02}.  Such analyzers
are by necessity conservative, and use approximations that are always
safe.  Analyzing more general code, e.g., with pointers, remains a
major challenge. For unsafe languages like C, correctness of the
analysis results is only guaranteed for well behaved programs; an
out-of-bounds array index can yield program behaviour not predicted by
the analysis. Consequently, it has proved difficult to parallelize
programs automatically, and most production codes are either written
in an explicitly parallel way or rely on speculative, run-time
parallelization techniques~\cite{PO03,CL03}.

In contrast, Embla
observes the actual data dependences that occur during program
execution, projects them onto relevant program parts, and interprets the
lack of a runtime data dependence as an indication that the program
parts involved are likely to be independent.
Developers will be responsible for selecting
program inputs that generate representative program executions with
good coverage. Section~\ref{sgcc} presents an analysis of how the
dependencies found varies with different inputs.

By construction, the methodology mentioned above preserves the
semantics and determinancy of the sequential program under the
assumption that all dependencies are found.  Should a dependence
remain undetected, it might manifest itself as a difference in
behavior between the parallel and sequential versions of the program
for some input. Given that the sequential program is deterministic,
rerunning it under Embla with the offending input will yield the
missing dependence.  Thus the programmining methodology supported by
Embla replaces the problem of finding synchronization errors in the
parallelized, nondeterministic, program with the problem of finding
all of the relevant dependencies in the sequential program.


In addition to data dependences, Embla can be extended to
detect I/O dependences that limit parallelism.  Another potential
extension is to measure the potential speed
improvement for parallelizing independent program parts and produce a
report with suggested program transforms that will yield the maximum
benefit.  These extensions are both future work.  This paper
presents the mechanism for collecting runtime data dependence information.

% End intro --------------------

% Begin using -----------------

\section{Using Embla}

\begin{figure} 
\small
\input{ex7.depgraph}
\caption{Example program with dependence graph} \label{ffirstex}
\end{figure}

To get a feeling for what dependence profiling is and what Embla can do, 
let us turn to the (contrived) example program in Figure~\ref{ffirstex},
where we see, from left to right, line numbers, data dependence 
arrows and source lines. 

A {\em data dependence} is a pair
of references, not both reads, to overlapping memory
locations with no intervening write. We will refer to these
references as the {\em endpoints} of the dependence.
For instance, in the figure, 
there is an arrow from line 13 to line 14 corresponding to
the assignment to {\tt q} (the {\em early} endpoint) followed by its use 
as an argument in {\tt inc(q)} (the {\em late} endpoint). Embla
internally distinguishes between flow (RAW), anti (WAR) and output (WAW) 
dependences, but we do not make that distinction in this paper. Embla 
can be instructed to show dependence types, which can be
useful for figuring out the reasons for individual dependences.

The endpoints of the dependence arrow discussed above are parts of
the code for {\tt main} itself, but Embla also tracks references made 
in function calls. For
instance, there is a flow dependence from line 14 to line 16
representing the write in the first invocation of {\tt inc} to the 
{\tt malloc}'d area pointed to by {\tt q} and the subsequent read 
of the same location by a later invocation of {\tt inc}. 
These dependences 
are reported as pertaining to {\tt main} rather than {\tt inc},
although the endpoints are part of the latter function. 
But the importance of the dependence is that, in {\tt main}, the calls
on line 14 and 16 can not be made in parallel.

The dependence given with a dotted arrow 
(from line 13 to line 18) is due to manipulation of administrative 
data structures by {\tt malloc}. If taken at face value such dependences will
serialize all calls to {\tt malloc}, but fortunately, the exact order
of memory allocations is not important. If the 
parallelized version of the program uses a thread safe 
implementation of {\tt malloc} these dependences are irrelevant and
can be ignored. Embla maintains a suppression list, with functions that behave 
in this way.  Similarly, dependences arise due to the
manipulation of the call stack, which are also irrelevant for parallelization.

% End using ------------------

% Begin algorithm -------------------

\section{The Dependence Attribution Algorithm}

\begin{figure} \small
\hrulefill
\[
\begin{picture}(160,60)(70,15)
\put(120,65){\makebox(60,10)[c]{\it A:\ \tt main}}
\put(150,65){\line(-2,-1){50}}
\put(150,65){\line( 0,-1){25}}
\put(150,65){\line( 2,-1){50}}
\put(95,45){\makebox(20,10)[r]{\it 14}}
\put(150,45){\makebox(20,10)[l]{\it 15}}
\put(185,45){\makebox(20,10)[l]{\it 16}}
\put(70,50){\makebox(20,10)[r]{\ldots}}
\put(210,50){\makebox(20,10)[l]{\ldots}}
\put(170,30){\makebox(60,10)[c]{\it D:\ \tt inc}}
\put(120,30){\makebox(60,10)[c]{\it C:\ \tt inc}}
\put(70,30){\makebox(60,10)[c]{\it B:\ \tt inc}}
\put(70,15){\makebox(60,10)[cb]{{\tt *q=}\ \ldots}}
\put(170,15){\makebox(60,10)[cb]{\ldots\ {\tt *q}\ \ldots}}
\end{picture}
\]
\hrulefill
\caption{Part of the execution tree of Example 1, edges are annotated 
with the line number of the corresponding call.} 
\label{ffextree}
\end{figure}

We are interested in dependences between program statements, and in 
this section we discuss how to compute these dependences efficiently from
the instruction level dependences directly observed by Embla.

\subsection{The execution tree}

Consider the {\em instruction trace} $S$ where each element corresponds 
to the execution of an instruction. From this trace we can construct a 
tree where the leaves are events from $S$, and each 
internal node is a call
event followed by a sequence of nodes and a return event. We call
this tree $T$ the {\em execution tree} of $S$. 
Each node corresponds to some execution of a procedure body.
Figure~\ref{ffextree} shows part of
the execution tree of Figure~\ref{ffirstex} where we have omitted the 
leaves. The nodes marked {\it B}, {\it C}, and {\it D} correspond to 
the three consecutive calls to {\tt inc} at lines 14--16 of {\tt main}.

The transformations we target will execute {\em siblings}, subtrees with
the same parent, in parallel (in Figure~\ref{ffextree}, {\it B}, {\it C} 
and {\it D} are siblings).  Hence, we 
are interested in dependences between siblings. These
arise from the 
dependences in the instruction trace; if $M$ and $N$ are siblings and
there is an instruction level dependence from an instruction in $M$ 
to an instruction in $N$ we have a (tree) dependence between $M$ and $N$
and we call $M$ the {\em source} and $N$ the {\em target} of the dependence.

In fact, each instruction level dependence yields exactly one dependence
between siblings in the execution tree since there is only one node
in the execution tree  
where the two events fall in two distinct children (siblings of each other)
that could potentially be rearranged. We call that node the {\em nearest
common ancestor} (NCA) of the endpoints of the instruction level dependence.
Note that either or both of the children could be a leaf, in which case the
corresponding dependence endpoint would be direct.
For example, in Figure~\ref{ffextree}
the dependence between the write in {\it B} and the read in {\it D} yields
only the tree dependence between the nodes {\it B} and {\it D} and the NCA of
{\it B} and {\it D} is {\it A}. 

The dependence is then reported as a dependence between the source lines
associated with the source and target subtrees, respectively; in the example
between lines 14 and 16 in {\tt main}.

\newcommand{\tracepile}{trace pile}

\subsection{Computing dependences} \label{snca}

Embla uses two main data structures: The {\em \tracepile}, which implements 
the execution tree, and the {\em memory table} which maps addresses to tree
nodes corresponding to the last write and subsequent reads of that
location. The \tracepile\ contains the part of the execution tree
corresponding to the part of the instruction trace that has been
seen so far.
For each reference, we look up the data address in the memory 
table. If the reference is a read, we use the previous write to generate
a flow dependence (RAW). If it is a write, we use the previous write to 
construct an output dependence (WAW) as well as all reads since that 
write to construct anti dependences (WAR).

If there are several reads with no intervening write, a subsequent write
(anti) depends on all of them. Since the reads do not depend on each other,
we need to keep track of all of them in the memory table to generate the
anti dependence edges explicitly. When that write has been processed 
the read list can be deallocated since
the write depends on all of the reads and
all subsequent references depend on the write.

The \tracepile\ contains the nodes of the execution tree in the same order
as in the instruction trace. For each node {\tt n}, {\tt n.line} is the 
source line associated with the instruction (leaf) or procedure call
(internal) 
corresponding to {\tt n}, {\tt n.parent} is the parent node in the execution tree 
and, if {\tt n} is 
on the path between the root node of the tree and the most recent 
event (leaf node), {\tt n.next} is the last child of {\tt n} (the node one step 
closer to the most recent one
along that path), so {\tt n.next.parent} = {\tt n}. This
path corresponds to the call stack; {\tt n.next} is the stack frame on
top of {\tt n}.

\begin{figure}
\small
\hrulefill
\begin{verbatim}
   DependenceEdge( oldEvent, currEvent ) {
       oldLine = oldEvent.line;
       ncaNode = oldEvent.node;
       while( ncaNode is not on stack ) {
           oldLine = ncaNode.line;
           ncaNode = ncaNode.parent;
       }
       if( ncaNode != currEvent.node )
           currLine = ncaNode.next.line;
       else
           currLine = currEvent.line;
       return ( oldLine, currLine );
    }
\end{verbatim}
\hrulefill
\caption{Constructing the dependence edge from the events corresponding
to a previous and a new reference}
\label{fdepedge}
\end{figure}    

For each instruction level dependence found using the memory table,
a source level dependence is computed using the algorithm in 
Figure~\ref{fdepedge}. Here we
have made use of the fact that the NCA must be part of the path from the
late instruction level endpoint to the root node in the execution tree. 
Thus
we can search from the early endpoint towards the root; the first node
on the stack is the NCA.

Path compression can be used to decrease the number of iterations of the 
{\tt while} loop.
Every node {\tt n} that is visited but is not on the stack can have its 
{\tt parent} field set
to its closest ancestor on the stack. We conjecture that this reduces the 
complexity of the algorithm to essentially constant time. 
% We can however do even better since the output of the tree dependence 
% calculation does only depend on the immediate
% descendants of nodes on the stack. Thus every subtree $T$ of the call tree, 
% including the events at its leaves, such
% that the root of $T$ is not on the stack can be represented by the root of $T$
% alone.


We can do better than path compression by {\em compacting} the \tracepile. 
Once a procedure call has returned, we will not distinguish between 
different events in the subtree corresponding to its (completed) 
execution. They will all be represented by the root node of the subtree.
We periodically compact the \tracepile, replacing subtrees
corresponding to completed calls by their root nodes. Since the 
memory table contains pointers into the \tracepile, compaction
entails forwarding these pointers to the root nodes of the compacted 
subtrees.
After compaction, the \tracepile\ contains the
tree nodes corresponding to the stack and their immediate children,
with non-leaf subtrees abridged to just the call and return events.

After forwarding, pointers to previously distinct events in the same read list 
now may point at the same 
tree nodes. In this case it is unnecessary to represent more than one 
copy of each pointer, thus compacting the read lists. This optimization is 
crucial in practice.

\subsection{Prototype implementation}

Embla is based on instrumented execution of binary code. Although
our examples of profiling output use a high level language (C),
the profiling itself is on the instruction level, followed by 
mapping the information to the source level using debugging information 
in the standard way.

Embla uses the Valgrind instrumentation infrastructure, so there
is no offline code rewriting; the Embla tool behaves like an emulator
of the hardware.


% End algorithm -----------------------

\begin{table}
\begin{center} \newcommand{\sms}{\hskip0.4em}
\begin{tabular}{|@{\sms}l@{\sms}|r@{\sms}r|r@{\sms}r@{\sms}r|r@{\sms}r@{\sms}r|} \hline
 & & & \multicolumn{3}{|c|}{No RLC} & 
\multicolumn{3}{c|}{RLC} \\
Prog & \#L & \#sD & \#iD & RSz & T 
                              & \#iD & RSz & T \\ \hline
\tt ex & 17 & 15 & 3.7K & 12K & 0.8 & 3.7K & 12K & 0.8 \\
\tt fib & 22 & 7 & 32M & 14K & 6.7 & 32M & 14K & 6.9 \\
\tt qs & 79 & 83 & 82M & 39M & 24.7 & 82M & 35M & 22.7 \\
\tt mpeg & 6053 & 3330 & 3.3G & 2.0G & 2355 & 3.2G & 109M & 847 \\ \hline
\end{tabular}
\end{center}
RLC: Read List Compaction, 
\#L: \#non blank source lines, \#sD: \#source dependences, \#iD: 
\#instruction
leve dependences, RSz: max bytes for read lists, T: run time (s)
\caption{Some experimental results} \label{trex}
\end{table}

\section{Preliminary Experiments}

We have run some preliminary experiments to verify that the tool performs as
expected. Some results are reported in Table~\ref{trex}. The programs are 
{\tt ex}, our example from Figure~\ref{ffirstex}, {\tt fib}, a recursive
Fibonnaci implementation, {\tt qs}, a recursive quicksort implementation 
and {\tt mpeg}, an MPeg encoder~\cite{MPEG} encoding 10 frames.

It is interesting
to see that read list compaction has a very different effect on different 
programs depending on the frequency of long sequences of reads from the same 
location (the RSz columns), ranging from no difference for {\tt ex} and 
{\tt fib} to a 20x difference for {\tt mpeg}. We also see that the number of 
instruction 
level dependences (the \#iD columns) are affected since the read lists 
are shorter when a write occurs following compaction (however, the 
eliminated read list 
items would have yielded no new source level dependences).

We note that Embla finds the expected independence of the recursive calls 
in the recursive divide-and-conquer programs {\tt qs} and
{\tt fib} although {\tt qs} is an in-place version coded 
with pointers and pointer arithmetic, something that is well known to be 
difficult to deal with for static analyzers. We have also done naive
hand parallelization of these programs according to the information 
yielded by Embla and obtained the expected good speedup.

\begin{table*}
\[
\begin{tabular}{|l|rrrr@{\ }r|rrrr@{\ }r|rr@{\,}r|r|} \hline
Filter & \multicolumn{5}{c|}{\# Dependencies} 
     & \multicolumn{5}{c|}{Size of deltas}
     & \multicolumn{3}{c|}{Relative sizes (\%)}
     & \# $\emptyset$  \\
     & max & min & avg & total & \% of N
     & max & min & avg & total & \% of N
     & max & min & g-mean & \\ \hline \hline
\multicolumn{15}{|c|}{ All 403.gcc source files } \\ \hline
N & 57733 & 8070 & 37589  & 80876 &  & 492 & 0 & 74  & 6502 &  & 1.08 & 0.00 & 0.09 & 9 \\
T & 39654 & 5637 & 26654  & 58336 & 72.1  & 434 & 0 & 41  & 3630 & 55.8  & 1.33 & 0.00 & 0.06 & 14 \\
\hline \hline
\multicolumn{15}{|c|}{ {\tt c-parse.c}, {\tt combine.c}, 
  {\tt insn-recog.c}, and {\tt toplev.c} } \\
\hline
N & 13218 & 718 & 7099  & 22158 &  & 215 & 0 & 33  & 2885 &  & 1.92 & 0.00 & 0.24 & 15 \\
T & 6499 & 636 & 3657  & 12001 & 54.2  & 134 & 0 & 13  & 1154 & 40.0  & 2.96 & 0.00 & 0.18 & 28 \\
C & 5008 & 398 & 2978  & 7166 & 32.3  & 64 & 0 & 6  & 568 & 19.7  & 1.53 & 0.00 & 0.14 & 40 \\
CT & 3139 & 347 & 1901  & 4936 & 22.3  & 39 & 0 & 4  & 331 & 11.5  & 1.44 & 0.00 & 0.22 & 56 \\
\hline
\end{tabular}
\]
Filters are {\bf N}one, {\bf T}ransitive or {\bf C}ontrol flow, \# Dependencies and Size of deltas
give maximum, minimum and average sizes of $D_i$ and $D_i^{\ast}$, respectively, over $i\in \cal A$
as well as sizes of $\cup_{i \in \cal A} D_i$ and $\cup_{i \in \cal A} D_i^{\ast}$. \% of N gives 
the total for the row as a percentage of the total of the N row. Relative sizes give the maximum, 
minimum and geometric mean size of $D_i^{\ast}$ as a percentage of $D_i$. Finally, \#~$\emptyset$
gives the number of inputs that yielded empty $D_i^{\ast}$.
\caption{Dependence statistics} \label{tdepstat}
\end{table*}

\subsection{Analysis of dependencies in GCC} \label{sgcc}

We are ultimately interested in knowing how well the dependencies
collected during testing predict the dependencies encountered in
production runs.  Since the sequential program we test is
deterministic, the issue becomes how well the behaviour under the
testing inputs predict the behaviour under other inputs.

To shed some light on this issue, we have studied the SPEC CPU 2006
benchmark 403.gcc, a variant of the popular open source Gnu C
Compiler. This is a very challenging program since, being an
optimizing compiler, it essentially looks for a large set of patterns
in its input an applies different code to different patterns.

We have collected flow (RAW) data dependencies for 88 different files
(6--6328 lines of pre-processed C code) drawn from the code base of
403.gcc itself. These dependencies do not include dependencies due to
{\tt malloc} and friends.

Each dependence is a triple $({ f}, { l}_1 \leftarrow { l}_2)$
meaning that $ l_1$ in source file $ f$ depends on line
$ l_2$ in the same file (recall that a dependence is always a
pair of lines in the same function). We only look at forward dependencies where 
$l_1>l_2$. Backward dependencies occur only in loops and the current
Embla prototype does not handle loop-carried dependencies adequately.
In particular, Embla does not recognize induction variables, hence
reporting all loops as having loop-carried dependencies. Loop bodies 
may still be parallelized using only forward dependencies.

The 88 inputs yield a total of 80876 dependencies, which
can be compared to the 484930 C source lines (excluding headers) of
this build of GCC. Evidently, the dependencies are rather sparse due
in part to the large number of lines not containing memory references at all,
and the number of unique source lines that occur in some dependence is
49196 (hence lines that occur in dependencies do so in on the average
about 1.6 lines, another measure of their sparseness).

Since we do not know the set of all dependencies that could be
provoked by some input, we give dependency deltas, sets of
dependencies that are generated by some input but not with some
reference set of inputs, rather than presenting coverages as
percentages of all dependencies. Thus, if the dependencies found with input data set $i$ is
$D_i$, we study \[D_j^I = D_j \setminus \bigcup_{i \in I} D_i\] for
some set of inputs $I$ not containing $j$. In particular, we have
$D_j^\ast = D_j^{{\cal A} \setminus j}$ where $\cal A$ is the set of
all the 88 inputs.

Table~\ref{tdepstat} shows some statistics on the dependencies found
in 403.gcc. From the relative sizes columns we see that at most about 1\% of the 
dependencies in any $D_i$ are preserved in $D_i^{\ast}$; these are the dependencies 
that are only provoked by input $i$. The \#~$\emptyset$ column shows that 9 out of the
88 $D_i^{\ast}$ were empty, meaning that these inputs generated no dependencies
that were not generated by other inputs.

Looking at the dependencies in the $D_i^{\ast}$ sets, we noticed that many of them were 
transitively implied by others. That is, for many $(f, l_1 \leftarrow l_2) 
\in D_i^{\ast}$ there were $(f, l_1 \leftarrow l), (f, l \leftarrow l_2) \in D_i$ for some $l$.
These dependencies give the same constraints on parallelization and make $(f, l_1 \leftarrow l_2)$
redundant. The T filter removes these redundant dependencies, yielding a 28\% reduction in 
overall dependence numbers and a 44\% reduction in the $D_i^{\ast}$ sets.

We next noticed many data dependencies that were subsumed by control
dependencies (which Embla currently does not capture). To get an idea
of the magnitude of this contribution, we implemented an approximate
control flow analysis by hand for four modules of 403.gcc that were
top contributors to the $D_i^{\ast}$ sets and appeared to have
nontrivial control flow. We also moved dependence endpoints up in the
syntax tree, e.g. from a branch in an if statement to the if statement
itself in a fashion analogous to the NCA computation discussed in
section~\ref{snca}.

The results are given in the four bottom rows of Table~\ref{tdepstat}
where we focus exclusively on dependencies $(f,l_1 \leftarrow l_2)$ where f is
one of the four modules. We see from the first row that these modules
account for almost half of the size of the $D_i^{\ast}$ sets. Looking
at the rightmos column, we see that control flow filtering alone makes
almost half of the $D_i^{\ast}$ sets empty, and that together with
transitive filtering almost two thirds are eliminated.


\begin{figure*}
\input{plot1} \input{plot2}
\caption{Coverage deltas (x-axis) versus dependence deltas (y-axis)
for no (left) or CT filtering} \label{fplot}
\end{figure*}

To account for the remaining $D_i^{\ast}$ sets we turned to coverage
analysis using the {\tt gcov} tool. We form $L_i$ and $L_i^{\ast}$
sets containing lines executed in the same way as for dependencies
above. Figure~\ref{fplot} plots the size of $L_i^{\ast}$ along the
x-axis and $D_i^{\ast}$ along the y-axis. The left hand plot shows the
unfiltered data for all modules wheras the right hand one shows the
data for the CT-filtered four modules. The left hand plot shows a very
clear correlation between execution coverage and dependendence
coverage wheras the right hand plot is less conclusive; it will be
interesting to see the effect of control flow filtering of all
modules. However, of the 30 inputs that yielded empty $L_i^{\ast}$ sets, only one
had a nonempty (actually singleton) $D_i^{\ast}$ set.



\section{Conclusion}

We have presented Embla, a data dependence profiling tool, discussed
the main issues and the algorithms used to resolve them. We argue that
data dependence profiling is a useful and practical complement to
static analysis for parallelization of new and legacy code. Finally,
we have reported on a few initial experiments with Embla showing a
strong correlation between executing the same part of a program and
generating the same dependencies.

In the future we plan extensions and refinements of Embla as well as
the application to real legacy code and 
more parallelization experiments using parallel environments such as
OpenMP.


\bibliographystyle{plain}
\bibliography{embib}

\end{document}
