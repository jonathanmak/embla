% -*- eval: (local-set-key "\M-q" 'undefined) -*-
%
% The line above will probably make Emacs ask if it is ok to evaluate the
% expression.  Answer y.

\section{Introduction}

Parallel programming is no longer optional.  To enjoy continued
performance gains with future generation multicore processors,
application developers must parallelize all software, old and new
\cite{TEL95,ONHWC96,KAB03,Sutter05}.  For scalable parallel
performance, program execution must be divided into large numbers of
independent tasks that can be scheduled on available cores and
hardware threads by runtime systems, such as thread libraries and
fork-join frameworks \cite{}.  For some classes of programs, automatic
parallelization is feasible, but with the current state-of-the-art,
most software requires manual parallelization \cite{}.  Our work
adresses the problem of assisting developers in this process, by
identifying the potential for parallelism in programs, and with
providing efficient tool support for this task.  In this paper, we
present a data dependence profiling approach to the parallelization
problem, an efficient algorithm to project data dependencies onto
relevant parts of the program code, and its implementation, the tool
Embla.

We will focus on parallelization by introducing procedure-level
fork-join parallelism.  To a first approximation, this means executing
procedure calls asynchronously.  Consider the program fragment below:
\begin{alltt}
   p();
   q();
   r();
\end{alltt}
Suppose that the calls to {\tt p()} and {\tt q()} are independent,
i.e., that neither procedure call writes to a memory location that the
other call accesses.  In this case, the call to {\tt p()} can be
executed asynchronously, by a different thread, and in parallel with
the call to {\tt q()}.  Suppose further that {\tt p()} and {\tt r()}
are not independent, so {\tt r()} can not be executed until the call
{\tt p()} is completed.  We can now rewrite the program as
\begin{alltt}
   spawn p();
   q();
   sync;
   r();
\end{alltt}

where we assume the availability of constructs {\tt spawn} to start
the call in parallel and {\tt sync} to wait for all {\tt spawn}'d
acivities to terminate (cf. \cite{BJKLR96,frigo98implementation}).

This style of parallelism can be implemented efficiently and is easy
to understand.  In particular, as long as {\tt p()} and {\tt q()} are
independent, the parallel program will produce identical results to
the sequential version.  Therefore it is sufficient to understand
(debug, verify, \ldots) the sequential program; everything except
performance carries over to the parallel version.

The price for this approach is that one must find independent
procedure calls (program fragments, in general).  Thus there must be
such calls in the code and the programmer must be able to determine
that they are in fact independent.  The availability of independent
calls in the program depends on the algorithms used and can be further
limited by sequential programming artifacts (such as re-use of
variables and sequential book-keeping in an otherwise parallelizable
algorithm).  Data dependence information can potentially help
indentify and remove such obstacles to parallel execution, but this
will not be further discussed here.

Parallelizing compilers mostly target loop parallelization based on
static data dependence analysis methods \cite{}.  Such analysers must
by necessity be conservative.  Since they are static, they must use
approximations which are always safe.  Good precision comes at a high
computational cost, especially when analysing large programs.
Analysing more general code, e.g., with pointers, remains a major
challenge \cite{}.  Consequently, it has proved difficult to
parallelize programs automatically in practice, and most production
codes are written in an explicitly parallel way.

In contrast, data dependence profiling observes the actual
dependencies that occur during program execution.


  Embla will
observe the actual data dependencies that occur during program
execution, project them onto relevant program parts, and interpret the
lack of a runtime data dependence as an indication that the program
parts involved are likely, but not guaranteed, to be independent.

is a heuristic method, aimed at
helping developers identify relevant parts of programs.

  As
in program testing, developers will be responsible for selecting
program inputs that generate representative program executions with
good coverage.

In the following sections, we will present, in Section 2, the Embla
model of data dependece in programs, in Section 3, the Embla algorithm
for projecting data dependencies onto program framgents, in Section 4,
the implementation of the Embla tool, based on Valgrind, in Section 5,
related work, and in Section 6, conclusions and future work.
