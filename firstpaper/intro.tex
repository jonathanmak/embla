\section{Introduction}

As wire delays and thermal problems have made improvements in clock
frequency much more difficult to realize, and increased levels of
instruction level parallelism faces problems of design complexity and
control dependencies, improving microprocessor performance has come to
depend increasingly on thread level parallelism. Multicore chips, also
known as CMPs (chip multiprocessors) pack several processor cores on a
single die, typically with per core L1 caches but sharing the L2
cache. Processor cores can also be multithreaded, executing
instructions from several threads simultaneously. Collectively 
these techniques can be referred to as {\em CMT} (chip multithreading).

From the point of view of the software, a CMT processor looks just
like a traditional shared memory symmetric multiprocessor. In particular, a CMT
program consists of multiple threads with shared state. 
Unfortunately, such code is much
harder to write than single threaded code, both because there are many
more program states to consider and because the relative execution
speed of threads are, in general, nondeterministic.  Not only is it
difficult to write multithreaded programs; most existing code is
single threaded (with the exception of code for numeric computation
and server oriented code).

We think that a practical way of resolving this issue is through hand
parallelization of sequential code. This method would both apply to
the existing code base and to newly developed code. This paper
presents a tool, Embla, to assist in this process.


While the dependence information that Embla finds can be used to 
support different models of parallelism, 
Embla is initially aimed mainly at supporting procedure-level 
fork-join parallelism. To a first approximation, this means executing 
procedure calls asynchronously. Consider the program fragment below:
\begin{alltt}
   foo();
   bar();
   baz();
\end{alltt}
Suppose that {\tt foo()} and {\tt bar()} are {\em independent}, ie that 
executing {\tt bar()} before {\tt foo()} does not change the meaning of
the program\footnote{There are many ways to formally give meaning to a
program, although it has never been done for C.}. A sufficient condition 
for independence
in this case is that {\tt foo()} and {\tt bar()} do not do any I/O and
that neither call writes to a memory location that the other call 
accesses. In that case the call to {\tt foo()} can be executed
asynchronously, 
by a different thread, and in parallel with the call to {\tt bar()}.
Suppose further that {\tt foo()} and {\tt baz()} are not independent,
so {\tt baz()} can not be executed until the call {\tt foo()} is completed.
We can express this as 
\begin{alltt}
   spawn foo();
   bar();
   sync;
   baz();
\end{alltt}
where {\tt spawn} starts the call in parallel and {\tt sync} waits
for all {\tt spawn}'d acivities to terminate (here we have borrowed 
programming constructs from the Cilk programming language 
\cite{frigo98implementation}).

This style of parallelism can be implemented efficiently (as is done
for instance in the Cilk language) and is easy to understand although
not as powerful as general thread parallelism with arbitrary
synchronization. In general, we can find more parallelism in a program 
by using the latter, but in many cases, fork-join paralleism suffices 
(especially for CMT processors which are not massively parallel).
In particular, as long as {\tt foo()} and {\tt bar()}
are independent, the behavior of the parallel program is identical to
the behavior of the sequential version. Therefore it is sufficient to
understand (debug, verify, \ldots) the sequential program; everything
except performance carries over to the parallel version.

The price for this approach is that one must find independent
procedure calls (program fragments, in general). Thus there must be
such calls in the code and the programmer must be able to make sure
that they are in fact independent. The first issue depends of course
on the algorithms used in the program.

The second issue is typically
dealt with using static analysis tools as is done in parallelizing
compilers. Such analysers are however very complex and must by
necessity be conservative (since they are static, they must use
approximations which are always safe). Good precision also comes at a
high computational cost, especially when analysing large programs.
Consequently, it has proved difficult to parallelize programs 
automatically in 
practice, and most production codes are written in an explicitly 
parallel way.

Embla takes a different approach by observing program execution
directly and recording the dependencies that occur. This means that
Embla in effect is a testing tool which should make it easy to 
integrate into existing software development processes.



