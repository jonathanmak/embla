\documentclass{acm_proc_article-sp}

\usepackage{epic}
\usepackage{color}
\usepackage{verbdef}
\usepackage{alltt}

\newcommand{\comment}[1]{\textit{[ #1 ]}}
\newenvironment{comment_env}
  {\begin{itshape}}
  {\end{itshape}}

\begin{document}

\title{Embla -- Data Dependence Profiling for Parallel Programming }
\subtitle{Extended Abstract}
\author{Karl-Filip Fax\'en, Lars Albertsson, Konstantin Popov, Sverker Janson\\
       Swedish Institute of Computer Science\\
       Box 1263\\
       SE-164 29 Kista\\
       Sweden\\
       \{kff,lalle,kost,sverker\}@sics.se}
\date{}
\maketitle

\begin{abstract}

With the proliferation of multicore processors, there is an urgent need for
tools and methodologies supporting parallelization of existing
applications.  In this paper we present a novel tool for aiding
programmers in parallelizing programs. The tool, Embla, is based on the
Valgrind framework, and allows the user to
discover the data dependencies in a sequential program, thereby exposing
opportunities for parallelization.  Embla performs a dynamic analysis,
and records dependencies as they
arise during program execution.  It reports an optimistic view of
parallelizable sequences, and ignores dependencies that do not arise during
execution.  In contrast to static analysis tools,
which by necessity make conservative approximation, Embla is able to find
more parallelism in sequential programs, and relies on the programmer to
transform the program in a correct manner. 
Moreover, since the tool instruments the machine code of the program,
it is largely language independent. 

\end{abstract}

% Intro start ---------------------

\section{Introduction}

Parallel programming is no longer optional.  To enjoy continued
performance gains with future generation multicore processors,
application developers must parallelize all software, old and new
\cite{TEL95,ONHWC96,KAB03,Sutter05}.  For scalable parallel
performance, program execution must be divided into large numbers of
independent tasks that can be scheduled on available cores and
hardware threads by runtime systems, such as thread libraries and
fork-join frameworks \cite{}.  For some classes of programs, automatic
parallelization is feasible, but with the current state-of-the-art,
most software requires manual parallelization \cite{}. 
Our work aims to help developers find the potential for parallelism in 
programs, in particular by providing efficient tool support.
In this paper, we
present a data dependence profiling approach to the parallelization
problem, an efficient algorithm to project data dependencies onto
relevant parts of the program code, and its implementation, the tool
Embla.

\begin{figure}
\small
\hrulefill
\[
\begin{minipage}[t]{3cm}
\begin{alltt}
   p();
   q();
   r();
\end{alltt}
\end{minipage}
\begin{minipage}[t]{3cm}
\begin{alltt}
   spawn p();
   q();
   sync;
   r();
\end{alltt}
\end{minipage} 
\]
\hrulefill
\caption{Example of Fork/join parallelism.}
\label{fforkjoin}
\end{figure}

We will focus on parallelization by introducing procedure-level
fork-join parallelism.  Consider the program fragment in
Figure~\ref{fforkjoin} (left):
Suppose that the calls to {\tt p()} and {\tt q()} are independent,
but that the call to {\tt r()} depends on the earlier calls. Then
the call to {\tt p()} can be
executed in parallel with
the call to {\tt q()}, as shown to the right.
Here we assume the availability of a construct {\tt spawn} to start
the call in parallel and {\tt sync} to wait for all {\tt spawn}'d
acivities to terminate (cf. \cite{BJKLR96,frigo98implementation}).
% As long as {\tt p()} and {\tt q()} are
% independent, the parallel program will produce identical results to
% the sequential version.  Therefore it is sufficient to understand
% (debug, verify, \ldots) the sequential program; everything except
% performance carries over to the parallel version.

In order to use this approach, one must find independent
procedure calls.  The availability of independent
calls in the program depends on the algorithms used and can be further
limited by sequential programming artifacts, such as re-use of
variables and sequential book-keeping in an otherwise parallelizable
algorithm.  Data dependence information can potentially help
indentify and remove such obstacles to parallel execution, but this
will not be further discussed here.

Parallelizing compilers mostly target loop parallelization based on
static data dependence analysis methods \cite{}.  Such analysers are
necessity conservative, and use
approximations that are always safe.  
% Good precision comes at a high
% computational cost, especially when analysing large programs.
Analysing more general code, e.g., with pointers, remains a major
challenge \cite{}.  Consequently, it has proved difficult to
parallelize programs automatically, and most production
codes are written in an explicitly parallel way.

In contrast, Embla
observes the actual data dependencies that occur during program
execution, project them onto relevant program parts, and interpret the
lack of a runtime data dependence as an indication that the program
parts involved are likely to be independent.
Developers will be responsible for selecting
program inputs that generate representative program executions with
good coverage.

Should a dependency remain undetected, it might manifest itself as a 
difference in behaviour between the parallel and sequential versions of the
program for some input. Rerunning the sequential program under
Embla with the offending input will, however, yield the missing dependency.
Hence, debugging the parallel program is reduced to debugging the sequential 
program, in contrast to the case of ad hoc construction of multithreded
code.

In addition to data dependencies, and Embla could easily be extended to
deal with I/O dependencies that limit parallelism them. We will, however,
leave these extensions for future work.

% End intro --------------------

% Begin using -----------------

\section{Using Embla}

\begin{figure} 
\small
\input{ex7.depgraph}
\caption{Example program with dependency graph} \label{ffirstex}
\end{figure}

To get a feeling for what dependency profiling is and what Embla can do, 
let us turn to the admittedly contrieved program in Figure~\ref{ffirstex}
were we see, from left to right, line numbers, data dependency 
arrows and source lines. 

A data dependence is a pair
of references, not both reads, to overlapping memory
locations with no interveaning write. We will refer to these
references as the {\em endpoints} of the dependence.
For instance, in the figure, 
there is an arrow from line 13 to line 14 corresponding to
the assignment to {\tt q} (the {\em early} endpoint) followed by its use 
as an argument in {\tt inc(q)} (the {\em late} endpoint). Embla
internally distinguishes between flow (RAW), anti (WAR) and output (WAW) 
dependencies, but in the figures in this paper, we do not make this
distinction. Embla can be instructed to show dependency types, which can be
useful for figuring out the reasons for individual dependencies.

For each of the dependency arrows in the figure that 
we have dicussed up to now, the endpoints have been part of the 
code for {\tt main}
itself. Embla also tracks references made in function calls. For
instance, there is a flow dependence from line 14 to line 16
representing the write in the first invocation of {\tt inc} to the 
{\tt malloc}'d area pointed to by {\tt q} and the subsequent read 
of the same location by a later invocation of {\tt inc}. 
Note that these dependencies 
are reported as pertaining to {\tt main} rather than {\tt inc},
although the endpoints are part of the latter function. 
But the importance of the dependence is that, in {\tt main}, the calls
on line 14 and 16 can not be made in parallel.

The dependency given with a dotted arrow 
(from line 13 to line 18) is due to manipulation of administrative 
data structures by {\tt malloc}. If taken at face value such dependencies will
serialize all calls to {\tt malloc}, but fortunately, the exact order
of memory allocations is not important. If the 
parallelized version of the program uses a thread safe 
implementation of {\tt malloc} these dependencies are irrelevant and
can be ignored. Embla maintains a black list of functions that behave 
in this way (the {\tt malloc} family in this example).



% End using ------------------

% Begin algorithm -------------------

\section{The Dependence Attribution Algorithm}

\begin{figure} \small
\hrulefill
\[
\begin{picture}(160,60)(70,15)
\put(120,65){\makebox(60,10)[c]{\it A:\ \tt main}}
\put(150,65){\line(-2,-1){50}}
\put(150,65){\line( 0,-1){25}}
\put(150,65){\line( 2,-1){50}}
\put(95,45){\makebox(20,10)[r]{\it 14}}
\put(150,45){\makebox(20,10)[l]{\it 15}}
\put(185,45){\makebox(20,10)[l]{\it 16}}
\put(70,50){\makebox(20,10)[r]{\ldots}}
\put(210,50){\makebox(20,10)[l]{\ldots}}
\put(170,30){\makebox(60,10)[c]{\it D:\ \tt inc}}
\put(120,30){\makebox(60,10)[c]{\it C:\ \tt inc}}
\put(70,30){\makebox(60,10)[c]{\it B:\ \tt inc}}
\put(70,15){\makebox(60,10)[cb]{{\tt *q=}\ \ldots}}
\put(170,15){\makebox(60,10)[cb]{\ldots\ {\tt *q}\ \ldots}}
\end{picture}
\]
\hrulefill
\caption{Part of the call tree of Example 1, edges are annotated 
with the line number of the corresponding call.} 
\label{ffextree}
\end{figure}

% To be useful to the programmer, the instruction level dependencies detected 
% by Embla must be mapped to source level. As discussed above, the dependency
% that Embla reports is not always in the same function as the dependency endpoints
% Embla is designed to help programmers 
% rewrite source code, so we need to project 
% the dependencies discovered in the dynamic instruction stream onto the 
% source code in such a way as to inform the programmer about legal code 
% transformations. Since we are dealing with transformations on the level of
% procedure calls, we need to reflect the dependencies we find to that level 
% as well. That is, we always want to know the dependencies between different 
% lines in the same procedure.
We are interested in dependencies between program statements (lines), and in 
this section we discuss how to compute these dependencies efficiently from
the instruction level dependencies Embla directly observes.

\subsection{From dependency endpoints to dependency edges: the call tree}

Consider the {\em dynamic instruction stream} $S$ where each element 
corresponds to the execution
of an instruction. We can view this stream as a tree where the leaves are 
events that do not correspond to call or return instructions and each 
non-leaf node is a call
event followed by a sequence of nodes and a return event. We call
this tree the {\em execution tree} of $S$. 
Each node corresponds to some execution of a procedure body.
Figure~\ref{ffextree} shows part of
the execution tree of figure~\ref{ffirstex} where we have omitted the 
leaves. The nodes marked {\it B}, {\it C} and {\it D} correspond to 
the three consecutive calls to {\tt inc} at lines 14--16 of {\tt main}.

The transformations we target will execute {\em siblings} (sub trees with
the same parent, thus part of the same procedure activation) in parallel, hence we 
are interested in dependencies between siblings. These arise from the 
dependencies in the instruction stream; if $M$ and $N$ are siblings and
there is an instruction level dependency from an instruction in $M$ 
to an instruction in $N$ we have a (tree) dependency between $M$ and $N$
and we call $M$ the {\em source} and $N$ the {\em target} of the dependency.

In fact, each instruction level dependency yields exactly one dependency
between siblings in the execution tree since there is only one node
in the execution tree  
where the two events fall in two distinct children (siblings of each other)
that could potentially be rearranged. We call that node the {\em nearest
common anscestor} (NCA) of the endpoints of the instruction level dependency.
Note that either or both of the children could be a leaf, in which case the
corresponding dependency endpoint would be direct.
For example, in figure~\ref{ffextree}
the dependency between the write in {\it B} and the read in {\it D} yields
only the dependence between the nodes {\it B} and {\it D} and the NCA of
{\it B} and {\it D} is {\it A}. 

The dependence is then reported as a dependence between the source lines
associated with the source and target nodes, respectively; in the example
between lines 14 and 16 in {\tt main}.

\begin{figure}
\small
\hrulefill
\begin{verbatim}
   DependenceEdge( oldEvent, currEvent ) {
       oldLine = oldEvent.line;
       ncaNode = oldEvent.node;
       while( ncaNode is not on stack ) {
           oldLine = ncaNode.line;
           ncaNode = ncaNode.parent;
       }
       if( ncaNode != currEvent.node )
           currLine = ncaNode.nextOnStack.line;
       else
           currLine = currEvent.line;
       return ( oldLine, currLine );
    }
\end{verbatim}
\hrulefill
\caption{Constructing the dependence edge from the events corresponding
to a previous and a new reference}
\label{fdepedge}
\end{figure}    

The algorithm for computing the source level dependency given the 
instruction level dependency is given in Figure~\ref{fdepedge}. Here we
have made use of the fact that the NCA must be part of the path from the
late instruction level endpoint to the root node in the execution tree 
(this path corresponds to the stack). Since we discover the dependencies
in the order their late endpoints occur in the instruction stream, we
can easily keep track of which tree nodes correspond to the stack. Thus
we can search from the early endpoint towards the root; the first node
on the stack is the NCA.

In the algorithm, an 
{\tt Event} represents a leaf node and a {\tt Node} corresponds 
to a non-leaf node. An event {\tt e} has two attributes: {\tt e.line} 
is the source line corresponding to {\tt e} and {\tt e.node} is the tree node
that {\tt e} is part of. Similarly, if {\tt n} is a node, then 
{\tt n.line} is the source line associated with the procedure call 
corresponding to {\tt n}, {\tt n.parent} is the parent node in the execution tree 
and, if {\tt n} is in the stack, {\tt n.nextOnStack} is the node on top of 
{\tt n} in the stack ({\tt n.nextOnStack.parent} = {\tt n}).

Path compression can be used to decrease the number of iterations of the 
{\tt while} loop.
Every node {\tt n} that is visited but is not on the stack can have its parent set
to its closest ancestor on the stack. We conjecture that this reduces the 
complexity of the algorithm to essentially constant time. 
% We can however do even better since the output of the tree dependence 
% calculation does only depend on the immediate
% descendants of nodes on the stack. Thus every subtree $T$ of the call tree, 
% including the events at its leaves, such
% that the root of $T$ is not on the stack can be represented by the root of $T$
% alone.

\subsection{The state of a memory location: the memory table}

When computing the dependencies created by a memory reference, we need to 
map the data address of the reference to information about the previous
reference to that location. What information we need depends on whether 
the new refenece is a read or a write:
\begin{description}
\item[READ]
We need the latest write to construct a flow dependence (RAW).
\item[WRITE]
We need the latest write to construct an output dependence (WAW) as well as
all reads since that write to construct anti dependencies (WAR).
\end{description}
If there are several reads with no interveaning write, a subsequent write
(anti) depends on all of them. Since the reads do not depend on each other,
we need to keep track of all of them in the memory table to generate the
anti dependency edges explicitly. Since the write depends on them all and
all subsequent references depend on the write, the read list can then be
deallocated.

An important aspect of compaction is that when a subtree of the dynamic
call tree is compacted to be represented by its root node, the memory table
needs to be updated to avoid dangling pointers to eliminated events. All
pointers are then forwarded to point to the representative nodes. In this
process, pointers to previously distinct events now may point at the same 
tree nodes. In this case it is unnecessary to represent more than one 
copy of each pointer, thus compacting the read lists. This optimization is 
crucial in practice.



\begin{description}
\item[The memory table:]
Contains, for each block of memory, pointers to events or nodes representing
the most recent write and the reads since that write. 
Blocksize is a tradeoff between memory use and precision; the examples in 
this paper were run with single byte blocks. 
\item[The trace pile:]
Contains trace records representing events and call tree nodes; thus the 
trace pile implements the call tree. Most trace records contain 
\begin{itemize}
\item
a pointer to a record containing information about the program 
address of the corresponding instruction (corresponds to the {\tt line} 
attribute in the previous discussion), and
\item
a pointer to the previous trace record representing a procedure call (the 
{\tt node} or {\tt parent} attributes of events and nodes, respectively).
\end{itemize}
\end{description}

If a dependency endpoint is part of the NCA (if the NCA is the current
frame or the frame pointed to from the memory table) it is labelled as
direct, otherwise it is indirect. If any of the activation frames on
the path from an endpoint to the NCA is on the black list, we have a
weak endpoint.



% End algorithm -----------------------

\section{Implementation and Preliminary Experiments}

Embla is based on instrumented execution of binary code. Although
our examples of profiling output use a high level language (C),
the profiling itself is on instruction level, followed by 
mapping the information to source level using debugging information 
in the standard way.

Embla uses the Valgrind instrumentation infrastructure, so there
is no offline code rewriting; the Embla tool behaves like an emulator
of the hardware.

\section{Related Work}

The fork-join parallelism has been recognized as a simple yet widely
acceptable pattern for building efficient parallel applications, and used e.g. in
Cilk~\cite{BJKLR96}, the Java fork/join framework~\cite{Lea00}, the
Filaments package~\cite{LF00}, and {TAM}~-- a compiler-controlled
threaded abstract machine~\cite{CGSE93,GSC95}. 

Many years of research has been devoted to automatic parallelization
by compilers utilizing static analysis techniques, yet the common
consensus is that the goal is unattainable beyond certain limited
well-specified areas such as loop parallelization in scientific
computation. Static analysis techniques have been also tried for the
closely related topic of data race in multithreaded programs
(see~\cite{Rinard01,NAW06} for surveys), yet some
authors~\cite{Rinard01} believe that this approach is too complex, and
the ultimate way to control data races is to use augmented type systems
that can eliminated them in the first place.

Our approach has certain similarity to on-the-fly data race detection
systems (such as~\cite{MellorCrummey91,SBNSA97,HRY02}) that can be
usable in certain contexts depending on the amount of synchronization
through shared memory, and the amount of overhead caused by instrumentation that
can be tolerated by a particular application.

\bibliographystyle{plain}
\bibliography{embib}

\end{document}
