\section{Task Models} \label{smethod}

Reflecting the task models of popular parallel programming environments such as Cilk~\cite{blumofe96cilk},
Java~\cite{lea00java}, TBB~\cite{reinders07intel} and TPL~\cite{leijen07parallel},
we use function calls/returns and (optionally) loop bodies to delineate tasks.
The reasons for using function calls/returns are that functions represent a reasonable amount of usually self-contained work (finer-grain parallelism is largely exploited by superscalar processors), require minimal syntactic restructuring to exploit and have single entry/single exit control flow.
Loop bodies are considered as tasks in one of our variant models below.
Naturally other legitimate coarse-grain parallelism will be left out,
the extent of which we estimate with another variant model.

In all our models each function call is spawned as a task at its call site.
The calling thread can then continue to execute statements that follow the function call without waiting for the call to return,
until control reaches a statement that has a dependence on the call.
At this point the calling thread must \emph{synchronize} on the task,
i.e.\ wait for the task to complete before going any further.

Our tool extends Embla~\cite{embla:08}, a Valgrind-based profiler that is based
on these models and outputs data dependences between source lines in the same
function. Embla works by observing instruction-level dependences that arise
whenever two instructions access the same memory location and at least one of
them is a write operation, The source and target instructions of each
dependence are mapped to source lines within their {\em Nearest Common
Ancestor} function.

In Embla 2, we calculate inherent parallelism by constructing a \emph{dynamic dependence graph} for each function.
This is a directed acyclic graph $G=(V,E)$ where each node $v\in V$ corresponds to an \emph{instantiation},
or execution, of a \emph{line}\footnote{We would like to have a node corresponding to each execution of a \emph{statement}, but the gcc debug tables that Embla uses are per-line not per-statement.} of the function,
and an edge $(u,v)\in E$ means that $u$ must be completed before $v$ can begin.
Edges $u \rightarrow v$ are inserted for:
\begin{enumerate}
\item
Data dependences between an instruction in the dynamic call tree of $u$ to one
in that of $v$ (as observed by the original Embla infrastructure described
above). Write-after-read and write-after-write dependences on the stack are
ignored (these tend to be on easily privatizable variables), as well as all
dependences between known commutative library functions, e.g.\ \texttt{malloc}.
\item
Control dependences between $u$ and $v$.
\item \label{dep-enum-model}
Dependences related to the parallelization model, that is, the restructuring of the code that we allow.
E.g.\ to enforce the requirement that only function calls can be spawned as tasks, there is an incoming edge to each node from the last executed node that does not contain a function call.
This effectively linearizes the graph except at function call spawns, the only points where forks are allowed.
\end{enumerate} 
The \emph{cost} of a node $v$, $\mathit{cost}(v)$,
is the minimum time required to execute the instantiation.
In our model this is essentially the number of instructions executed on that line,
with the additional requirement that if one of the instructions is a function call,
then the length of the \emph{critical path} of that call is included in the
\emph{cost} of the node.
The \emph{critical path} $\mathit{CP}$ is the path in $G$ with the largest total cost.
The amount of inherent parallelism for a function call is then the cost of serial execution divided by the length of the critical path,
i.e. $\sum_{v\in V} \mathit{cost}(v)/\sum_{v\in \mathit{CP}} \mathit{cost}(v)$.
The inherent parallelism of a program is simply that of its \texttt{main} function.
This figure is the speed-up of the parallelized program given unlimited
processors and zero task creation overheads.

Potential optimizations are explored under variants of the baseline model:

\paragraph{Exact data dependences}
By default, data dependences are \emph{aggregated} per source line over a
program's execution and then applied to all instantiations of the line.  This
models the parallelism that is possible by inserting synchronization points in
source code, necessitating the same synchronization point for all
instantiations.  A variant is to use \emph{exact} dependences by allowing each
instantiation of the same line to have its own set of dependences as observed
during execution\footnote{Cf.\ the difference between context-sensitive and
context-insensitive static analysis}, modelling the effects of dynamic
techniques such as thread-level
speculation~\cite{Rundberg01anall-software,gregory05stampede}, in the ideal
case where the continuation of each task instance runs in parallel with the
task right up until the point where a conflict would have been detected.

\paragraph{Loop iterations as tasks}
As mentioned, there is an option to parallelize loops in Embla~2,
where an established algorithm \cite{aho86compilers} is used to identify natural loops,
iterations of which are spawned as tasks just like function calls.
Updates to the loop index, which are outside the task boundaries, are still serialized.

\paragraph{Reduction operations}
Reductions are accumulation operations on variables such as \texttt{acc += f(i);},
where the order in which instances of the associative reduction operation ({\tt+})
are executed makes no difference to the final value of the accumulator.
Embla 2 includes a helper program that statically identifies and annotates reduction
operations in loops,
dependences between which can be safely ignored by Embla to enable greater parallelism.

\paragraph{Spawn hoisting}
Another way of enabling further parallelism is by spawning tasks earlier (a form of code motion optimization).
In this variant, function calls are spawned as early as dependences allow,
rather than only when control in the calling thread reaches the call site.
This is modelled by relaxing constraint~\ref{dep-enum-model} above
so that the incoming edge is only inserted to each node that does not contain a
function call.

\paragraph{Line-level parallelism}
By only considering function calls and potentially loop iterations as tasks, other forms of task-level parallelism are naturally excluded.
To see how much more parallelism there is, we introduce a variant model where all lines can be spawned, regardless of whether they contain function calls.
This is modelled by not applying constraint~\ref{dep-enum-model} at all.
Note however that while this allows us to see more parallelism, some of the extra parallelism discovered will be too fine-grain to be exploitable.




































% \section{Models of Parallelism} \label{smethod}
% 
% One of the main aims of our tool is to separate out task-level parallelism from instruction-level parallelism in a program.
% Instruction-level parallelism is fine-grain, local and typically exploited in superscalar and VLIW machines.
% It is however unsuitable for multi-core processors, as threading and communication overheads would dominate.
% Thus we would like to only consider parallelism at \emph{task}-level, where a task is a set of instructions that can run independently of other concurrent tasks, and is large enough to justify the costs of spawning a new thread to execute it.
% 
% To mirror the task model of popular parallel programming environments such as Cilk~\cite{blumofe96cilk}, Java~\cite{lea00java}, TBB~\cite{reinders07intel} and TPL~\cite{leijen07parallel}, we use function calls and returns to delineate tasks.
% The main reason for this is that functions are mainly used by programmers as an abstraction over a sequence of operations, and as such tend to be relatively self-contained and independent and would typically consist of a substantial amount of work.
% Also, the use of existing programming constructs such as function calls and returns as task boundaries means that the task-level parallelism that we discover can be extracted by both programmer and compiler easily.
% 
% In our model, each function call is spawned at its call site as a task.
% The calling thread can then continue to execute statements that follow the function call without waiting for the call to return, until control reaches a statement that has a dependence on the call.
% At this point the calling thread must \emph{synchronize} on the task, i.e.\ wait for the task to complete before going any further.
% 
% \begin{figure}
% \small
% \hrulefill
% \[
% \begin{minipage}[t]{3cm}
% \begin{alltt}
%    p();
%    q();
%    r();
% \end{alltt}
% \end{minipage}
% \begin{minipage}[t]{3cm}
% \begin{alltt}
%    spawn p();
%    spawn q();
%    sync;
%    r();
% \end{alltt}
% \end{minipage}
% \]
% \hrulefill
% \caption{An example of function-call spawning and synchronization, using Cilk-like syntax}
% \label{fforkjoin}
% \end{figure}
% 
% As an example of this, consider the program fragment in
% Figure~\ref{fforkjoin} (left):
% Suppose that the calls to {\tt p()} and {\tt q()} are independent,
% but that the call to {\tt r()} depends on the earlier calls. Then
% the call to {\tt p()} can be
% executed in parallel with
% the call to {\tt q()}, as shown on the right.
% Here we assume the availability of the constructs {\tt spawn} to start
% the call in parallel and {\tt sync} to synchronize on all previously {\tt spawn}'ed
% tasks (cf.\ Cilk).
% %\cite{BJKLR96}
% 
% % As long as {\tt p()} and {\tt q()} are
% % independent, the parallel program will produce identical results to
% % the sequential version.  Therefore it is sufficient to understand
% % (debug, verify, \ldots) the sequential program; everything except
% % performance carries over to the parallel version.
% 
% As in Cilk, each function call must synchronize on all tasks it has spawned that are still running before returning, meaning that all tasks must properly nest.
% While such a design choice may result in some potential parallelism being lost, it preserves program modularity by ensuring that each task is responsible for synchronizing on the tasks it has spawned.
% 
% We calculate inherent parallelism by constructing a \emph{dynamic dependence graph} for each function call, an example of which is shown in Figure~\ref{example-depgraph}.
% A dynamic dependence graph is a directed acyclic graph $G=(V,E)$ where each node $v\in V$ corresponds to an \emph{instantiation}, or execution, of a \emph{line}\footnote{Ideally we would like to have a node corresponding to each execution of a \emph{statement}, but the gcc debug information that Embla uses contains only line not statement information.} of the program, and each edge $e\in E$ corresponds to a dependence between two line instantiations.
% Such a dependence means that they cannot be executed in parallel---the source of the dependence must be completed before the target can begin.
% The \emph{cost} of a node $v$, $\mathit{cost}(v)$, is the minimum time required to execute the instantiation.
% In our model this is basically the number of instructions executed on that line, with the additional requirement that if one of the instructions is a function call, then the length of the critical path of that call is included in the cost of the node.
% The \emph{critical path} $\mathit{CP}$ is the path in $G$ with the largest total cost.
% The amount of inherent parallelism for a function call is then the cost of serial execution divided by the length of the critical path, i.e. $\sum_{v\in V} \mathit{cost}(v)/\sum_{v\in \mathit{CP}} \mathit{cost}(v)$.
% The inherent parallelism in the entire execution of a program is then the inherent parallelism for its \texttt{main} function.
% 
% \begin{figure}
%   \begin{center}
%   \small
%   \input{example.depgraph}
%   \end{center}
%   \caption{Example dependence graph from Embla}
%   \label{example-depgraph}
% \end{figure}
% 
% We now describe in greater detail some parameters of our model, which can be varied to fit different parallel programming models and to examine the effects of different optimizations.
% 
% \subsection{Data dependences}
% Data dependences arise when two lines of a program access the same memory location, and at least one of them writes to it.
% They can be categorized into true (read-after-write), anti- (write-after-read) and output (write-after-write) dependences.
% In our tool data dependences can be \emph{exact} or \emph{aggregated}.
% With exact dependences, each instantiation of a line has its own set of dependences---different instantiations of the same line may therefore have different dependences, e.g.\ if they access different memory locations.
% When data dependences are aggregated, however, there is just one set of dependences for each line, namely the union of the dependences for each instantiation\footnote{
% This distinction parallels that between {\em context-sensitive} and
% {\em context-insensitive} static analyses.
% Exact dependences are maximally context-sensitive in that line instantiations
% essentially include both the full call-stack and, for loops, the iteration
% count.}.
% This means that the dynamic dependence graph will be built with each instantiation of a line having the same set of dependences.
% The difference between the two can be illustrated in the program in Figure~\ref{datadeps}, where there is a dependence between the calls to \texttt{g1} and \texttt{g2} in \texttt{f} only when \texttt{a} and \texttt{b} alias.
% A model using aggregated data dependences would ascribe that dependence to all calls to \texttt{f}, resulting in no parallelism, while one using exact dependences would only place it in the second call, resulting in some parallelism in the first call.
% 
% \begin{figure}
%   \begin{center}
%   \small
%   \begin{verbatim}
%     void g1(int *a) {
%       ...
%       *a = ...; // Writes to *a
%     }
%   
%     void g2(int *b) {
%       ... = *b; // Reads from *b
%       ...
%     }
% 
%     int f(int *a, int *b) {
%       int x;
%       g1(a);
%       g2(b);
%     }
% 
%     int main(int argc, char *argv[]) {
%       int x=0, y=1;
%       f(&x, &y);
%       f(&x, &x);
%     }
%   \end{verbatim}
%   \end{center}
%   \caption{Program illustrating the difference between aggregated and exact data dependences}
%   \label{datadeps}
% \end{figure}
% 
% Aggregated dependences are used when we would like to consider parallelism that can be achieved by inserting synchronization points in source code, necessitating the same synchronization point for all instantiations of a line.
% Exact dependences, on the other hand, allow us to consider the parallelism that is possible by more dynamic techniques such as thread-level speculation (TLS)~\cite{Rundberg01anall-software,gregory05stampede,welc05safe}.
% This difference can again be illustrated in the example above.
% We cannot statically parallelize the calls to \texttt{g1} and \texttt{g2} as that would lead to a race condition in the second call of \texttt{f}.
% With TLS however, we can attempt to run \texttt{g1} and \texttt{g2} in parallel, which results in potential parallelism in the first call to \texttt{f}, at the expense of a possible performance penalty of a rollback in the second call.
% 
% \subsection{Control dependences}
% When whether a line is executed is not known until the execution of another (typically a branch), the former line is said to be control-dependent on the latter, and must not begin execution until the latter has completed.
% In our tool, control dependences are calculated by constructing Control Dependence Graphs~\cite{ferrante87program}.
% However, our model also allows control dependences to be excluded.
% This in effect gives us the parallelism achievable with perfect (100\% accuracy) control speculation.
% Previous studies on branch prediction and control speculation~\cite{smith98study,lam92limits} suggest that simple prediction algorithms can result in high accuracies for many programs, meaning that a model with control speculation is reasonable.
% 
% \subsection{Granularity}
% In order to restrict parallelism to be task-level only, our default model only spawns lines containing function calls.
% Lines with simple statements rather than function calls must be executed in order as they are usually too fine-grain.
% Naturally this also excludes other forms of task-level parallelism where tasks are not delineated by function calls and returns.
% As a result, our tool also contains an option to relax this constraint and spawn all lines, whether they contain function calls or not.
% While this allows us to see greater task-level parallelism, some of the parallelism discovered will be too fine-grain to be useful.
% Nonetheless, based on the line-level parallelism discovered, the programmer can then decide to refactor certain lines into their own spawnable function, once they have determined that the parallelism is sufficiently coarse-grain.
% 
% \subsection{Loop iterations}
% Loops are generally considered to be another source of task-level parallelism, and are the primary parallel programming construct in programming libraries and environments such as OpenMP~\cite{dagum98openmp} and TBB~\cite{reinders07intel}.
% There is an option to parallelize loops in our tool, where an established algorithm \cite{aho86compilers,muchnick97advanced} is used to identify natural loops, iterations of which can be spawned as tasks in the same way as function calls.
% 
% \subsection{Spawn hoisting}
% 
% \begin{figure}
%   \begin{center}
%   \scriptsize
%   \begin{SubFloat}{\label{spawn:orig}Original program extract}
%     \begin{minipage}{0.7in}
%       \begin{verbatim}
% int x, y, r;
% 
% r = 0;
% x = fib(n-1);
% r = r + x;
% y = fib(n-2);
% r = r + y;
% return r;
% 
% 
%       \end{verbatim}
%     \end{minipage}%
%   \end{SubFloat}%
%   \qquad
%   \begin{SubFloat}{\label{spawn:without}Best possible parallelization \emph{without} spawn hoisting}
%     \begin{minipage}{1.0in}
%       \begin{verbatim}
% int x, y, r;
% 
% r = 0;
% x = spawn fib(n-1);
% sync;
% r = r + x;
% y = spawn fib(n-2);
% sync;
% r = r + y;
% return r;
%       \end{verbatim}
%     \end{minipage}%
%   \end{SubFloat}%
%   \qquad
%   \begin{SubFloat}{\label{spawn:with}Best possible parallelization \emph{with} spawn hoisting}
%     \begin{minipage}{1.0in}
%       \begin{verbatim}
% int x, y, r;
% 
% x = spawn fib(n-1);
% y = spawn fib(n-2);
% r = 0;
% sync;
% r = r + x;
% r = r + y;
% return r;
% 
%       \end{verbatim}
%     \end{minipage}%
%   \end{SubFloat}%
%   \end{center}
%   \caption{An illustration of spawn hoisting on part of a function that calculates a Fibonacci number.}
%   \label{spawn}
% \end{figure}
% 
% In our default model, a task is only spawned when control reaches its original site.
% It is possible, however, that some tasks can be started earlier, resulting in greater parallelism.
% This is illustrated in Figure~\ref{spawn}, where in the original program the line immediately following each call to \texttt{fib} is dependent on the result of the call (Figure~\ref{spawn:orig}).
% As we need to synchronize on a spawned task before we reach any line that depends on it, we would need to insert a \texttt{sync} immediately after each call (Figure~\ref{spawn:without}).
% If we allow function spawns to be hoisted, however, more opportunities for parallelism exist, as shown in Figure~\ref{spawn:with}, where both calls to \texttt{fib} have been hoisted to the beginning of the program fragment.
% To examine the extra parallelism this optimization allows, our tool provides an option to allow each spawn to be hoisted to as early a point as dependences allow.
% 
